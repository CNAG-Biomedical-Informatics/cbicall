{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CBIcall","text":"Reproducible germline variant calling for WES, WGS, and mtDNA"},{"location":"#what-is-cbicall","title":"What is CBIcall?","text":"<p>CBIcall (CNAG Biomedical Informatics framework for variant calling) is a lightweight, reproducible germline variant-calling framework developed at CNAG. It wraps established best practices (BWA \u2192 GATK \u2192 VQSR or hard filters) into easy-to-run Bash and Snakemake workflows, enabling labs to produce high-quality single-sample and cohort VCFs with minimal effort. \ud83e\uddec</p>"},{"location":"#why-cbicall","title":"Why CBIcall?","text":"<ul> <li>Implements GATK best practices (GATK 4.6 and GATK 3.5 legacy modes), tuned for real project needs.</li> <li>Supports both single-sample and cohort pipelines (WES and WGS), using GenomicsDBImport or per-chromosome sharding.</li> <li>Includes robust mitochondrial DNA analysis via MToolBox with heteroplasmy-aware calling.</li> <li>Simple YAML configuration with sensible defaults for fast onboarding.</li> <li>Transparent logs and structured outputs suitable for QC, auditing, and downstream interpretation.</li> </ul>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Per-sample preprocessing: alignment, read groups, merging, duplicates, and BQSR.</li> <li>High-quality variant calling: per-sample gVCFs and scalable joint genotyping.</li> <li>Variant quality control: VQSR when possible, with reproducible hard-filter fallback.</li> <li>mtDNA pipelines: MToolBox-based assembly, annotation, and heteroplasmy estimation.</li> <li>Flexible deployment: Bash and Snakemake workflows; supports containerized environments.</li> <li>Utility tools: coverage estimation, sex determination, basic cohort QC metrics.</li> </ul> <p>\u27a1\ufe0f Get Started</p>"},{"location":"about/about/","title":"\ud83d\udc64 About","text":"<p>CBIcall has been developed at CNAG, Barcelona, Spain.</p>"},{"location":"about/about/#developers","title":"Developers","text":"CLIDocumentation <ul> <li>Manuel Rueda</li> </ul> <ul> <li>Manuel Rueda. </li> </ul> <p>Project documentation was created using Material for MkDocs.</p>"},{"location":"about/about/#acknowledgments","title":"Acknowledgments","text":"CNAG <ul> <li>Ivo G. Gut and his team</li> </ul>"},{"location":"about/citation/","title":"Citation for ClarID-Tools","text":"<p>Citation</p> <p>TBD</p> <p>Funding agencies</p> <ul> <li>TBD</li> </ul>"},{"location":"help/faq/","title":"Frequently Asked Questions","text":""},{"location":"help/faq/#wes-wgs","title":"WES / WGS","text":"What are the reference genomes used? <p>GRCh37 (b37) - GATK-compatible reference genome</p> <p>GRCh38 (hg38) - GATK-compatible reference genome</p> What are the capture kits for WES? <ul> <li> <p>For GATK version 3.5: Exome capture is based on Agilent SureSelect.</p> </li> <li> <p>For GATK version 4.6: Exome and WGS reference is based on the GATK bundle (b37).</p> </li> </ul>"},{"location":"help/faq/#last-change-2025-10-15-by-manuel-rueda","title":"last change 2025-10-15 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-10-15-by-manuel-rueda_1","title":"last change 2025-10-15 by Manuel Rueda","text":""},{"location":"help/faq/#mtdna-mtoolbox","title":"mtDNA (MToolBox)","text":"What is the reference genome used? <p>RSRS (rsrs) - Reconstructed Sapiens Reference Sequence</p> What does GT=1 mean in results? <p>In variant reports, the Genotype (GT) field shows the observed allele using VCF allele indices:</p> <ul> <li><code>0</code> = reference allele</li> <li><code>1</code> = first alternate (ALT) allele</li> <li><code>2</code>, <code>3</code>, ... = additional ALT alleles (multiallelic)</li> </ul> <p>For chrM/MT (mtDNA), callers typically encode genotypes as haploid (not allele pairs).</p>"},{"location":"help/faq/#last-change-2025-10-15-by-manuel-rueda_2","title":"last change 2025-10-15 by Manuel Rueda","text":""},{"location":"help/faq/#meaning","title":"Meaning","text":"<ul> <li><code>GT = 1</code> \u2192 ALT allele detected in that sample</li> <li>No <code>/</code> or <code>|</code> separator because only one allele index is stored</li> <li>Biological interpretation relies on:<ul> <li><code>HF</code> \u2192 heteroplasmy fraction (molecules supporting ALT)</li> <li><code>DP</code> \u2192 read depth (total support)</li> </ul> </li> </ul>"},{"location":"help/faq/#examples","title":"Examples","text":"GT Interpretation (mtDNA) <code>0</code> Only reference allele observed <code>1</code> ALT allele present (homoplasmic or heteroplasmic, check <code>HF</code> + <code>DP</code>) <code>0/1</code>, <code>1/2</code> (rare) Multiallelic call, still haploid encoding \u2014 not diploid zygosity <p>TL;DR: <code>GT = 1</code> = ALT detected. Check <code>HF</code> and <code>DP</code> for biology.</p> <p>Tip</p> <p>For mtDNA, <code>GT</code> tells you which allele, not how much.</p> <p>Use <code>HF</code> + <code>DP</code> to interpret heteroplasmy or homoplasmy.</p>"},{"location":"help/faq/#last-change-2025-10-15-by-manuel-rueda_3","title":"last change 2025-10-15 by Manuel Rueda","text":""},{"location":"help/faq/#general","title":"General","text":"How do I set up <code>cbicall</code> to work on an HPC system? <p>To adapt <code>cbicall</code> for your HPC environment, update the file <code>workflows/bash/gatk_3.5/parameters.sh</code> so that it reflects your local module system, paths, and resource settings.</p> <p>Below is the configuration used at CNAG-HPC, which you can use as a template:</p> <pre><code># $VERSION taken from CBICall\n\n# Paths\nDATADIR=/media/mrueda/2TBS\n#DATADIR=/cbicall-data  # From inside the container\nDBDIR=$DATADIR/Databases\nNGSUTILS=$DATADIR/NGSutils\n\n# Environment\nexport TMPDIR=$DATADIR/tmp\nexport LC_ALL=C\nexport GATK_DISABLE_AUTO_S3_UPLOAD=true   # disable unintended S3 uploads\nexport ARCH=$(uname -m)\n\n# Genome selection (default b37)\n: \"${GENOME:=b37}\"\n\n# Memory &amp; architecture\nMEM=8G\nMEM_GENOTYPE=64G\n\n# Java &amp; tool binaries per architecture\nif [ \"$ARCH\" == \"aarch64\" ]; then\n    export JAVA8=/usr/lib/jvm/java-8-openjdk-arm64/bin/java\n    BWA=$NGSUTILS/bwa-0.7.18_arm64/bwa\n    SAM=$NGSUTILS/samtools-0.1.19_arm64/samtools\n    BED=$NGSUTILS/bedtools2_arm64/bin/bedtools\n    # Mtoolbox bundled binaries do not work with aarch64\n    # PY27_PREFIX=$NGSUTILS/python_2.7/linux-aarch64/Python-2.7.18\nelse\n    export JAVA8=$NGSUTILS/java8/amazon-corretto-8.472.08.1-linux-x64/bin/java\n    BWA=$NGSUTILS/bwa-0.7.18/bwa\n    SAM=$NGSUTILS/samtools-0.1.19/samtools\n    BED=$NGSUTILS/bedtools2/bin/bedtools\n    PY27_PREFIX=$NGSUTILS/python_2.7/linux-x86_64/python27_portable\n    # Python 2 module to use on HPC (fallback)\n    PY27_MODULE=\"Python/2.7.18-GCCcore-11.2.0\"\nfi\n\n# Picard (shared by GATK3 &amp; bed conversion)\nPIC=\"$JAVA8 -Xmx$MEM -Djava.io.tmpdir=$TMPDIR -jar $NGSUTILS/picard-2.25/build/libs/picard.jar\"\n\n# GATK 3.5 (legacy)\nGATK=\"$JAVA8 -Xmx$MEM -Djava.io.tmpdir=$TMPDIR -jar $NGSUTILS/gatk/gatk-3.5/GenomeAnalysisTK.jar\"\n\n# GATK 4+ launcher (recommended)\nGATK4_BIN=\"$NGSUTILS/gatk/gatk-4.6.2.0/gatk\"\nGATK4_JAVA_OPTS=\"--java-options -Xmx${MEM}\"\nGATK4_JAVA_OPTS_64G=\"--java-options -Xmx${MEM_GENOTYPE}\"\n\n# MToolBox directory and DB\nMTOOLBOXDIR=$NGSUTILS/MToolBox-master/MToolBox\nMTOOLBOXDB=$DBDIR/mtDNA\n\n############################################\n# Reference bundle &amp; resources by GENOME\n############################################\nif [ \"$GENOME\" = \"hg38\" ]; then\n    # GATK bundle &amp; reference (hg38) - WGS only\n    BUNDLE=$DBDIR/GATK_bundle/hg38\n\n    REF=$BUNDLE/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta\n    REFGZ=$REF   # not gz in this bundle; keep var for compatibility\n    REF_DICT=$BUNDLE/resources_broad_hg38_v0_Homo_sapiens_assembly38.dict\n\n    # Known-sites / resources (hg38)\n    dbSNP=\"$DBDIR/dbSNP/human_9606_b146_GRCh38p2/All_20160407.renamed.vcf.gz\"\n    MILLS_INDELS=$BUNDLE/resources_broad_hg38_v0_Mills_and_1000G_gold_standard.indels.hg38.vcf.gz\n    KG_INDELS=$BUNDLE/resources_broad_hg38_v0_Homo_sapiens_assembly38.known_indels.vcf.gz\n    HAPMAP=$BUNDLE/resources_broad_hg38_v0_hapmap_3.3.hg38.vcf.gz\n    OMNI=$BUNDLE/resources_broad_hg38_v0_1000G_omni2.5.hg38.vcf.gz\n\n    # No WES intervals/capture on hg38 (WGS only)\n    EXOME_BED=\"\"\n    INTERVAL_LIST=\"\"\n    EXOM=\"\"\n\nelse\n    # Default b37\n    BUNDLE=$DBDIR/GATK_bundle/b37\n    REF=$BUNDLE/references_b37_Homo_sapiens_assembly19.fasta\n    REFGZ=$BUNDLE/references_b37_Homo_sapiens_assembly19.fasta.gz\n    REF_DICT=$BUNDLE/references_b37_Homo_sapiens_assembly19.dict\n\n    # Variant resources\n    dbSNP=$DBDIR/dbSNP/human_9606_b144_GRCh37p13/All_20160408.vcf.gz\n    MILLS_INDELS=$BUNDLE/b37_Mills_and_1000G_gold_standard.indels.b37.vcf.gz\n    KG_INDELS=$BUNDLE/b37_1000G_phase1.indels.b37.vcf.gz\n    HAPMAP=$BUNDLE/b37_hapmap_3.3.b37.vcf.gz\n    OMNI=$BUNDLE/b37_1000G_omni2.5.b37.vcf.gz\n\n    # Exome targets\n    EXOME_BED=$BUNDLE/b37_Broad.human.exome.b37.bed\n    INTERVAL_LIST=$BUNDLE/b37_Broad.human.exome.b37.interval_list\n\n    # Agilent SureSelect Whole Exome (your current setup)\n    EXOM=$DBDIR/Agilent_SureSelect/hg19/bed\nfi\n\n# Training sets for VQSR\nSNP_RES=\"-resource:hapmap,known=false,training=true,truth=true,prior=15.0 $HAPMAP \\\n         -resource:omni,known=false,training=true,truth=false,prior=12.0 $OMNI \\\n         -resource:dbsnp,known=true,training=false,truth=false,prior=6.0 $dbSNP\"\nINDEL_RES=\"-resource:mills,known=true,training=true,truth=true,prior=12.0 $MILLS_INDELS\"\n\n# Joint variant calling\nBATCH_SIZE=50\nMIN_SNP_FOR_VQSR=1000\nMIN_INDEL_FOR_VQSR=8000\n\n# UnifiedGenotyper parameters (legacy)\nDCOV=1000\nUG_CALL=50\nUG_EMIT=10\n</code></pre> Do you have an example in how to run <code>cbicall</code> in Slurm HPC? <pre><code>#!/bin/bash\n#\n# run_cbicall_slurm.sh\n# usage: ./run_cbicall_slurm.sh &lt;sample_id&gt; &lt;pipeline: wes|wgs&gt;\n\nif [ \"$#\" -ne 2 ]; then\n  echo \"Usage: $0 &lt;sample_id&gt; &lt;pipeline: wes|wgs&gt;\"\n  exit 1\nfi\n\nSAMPLE_ID=$1\nPIPELINE=$2\n\nif [[ \"$PIPELINE\" != \"wes\" &amp;&amp; \"$PIPELINE\" != \"wgs\" ]]; then\n  echo \"Error: pipeline must be 'wes' or 'wgs'\"\n  exit 1\nfi\n\n# choose SLURM settings based on pipeline\nif [ \"$PIPELINE\" = \"wes\" ]; then\n  QUEUE=\"normal\"\n  TIME=\"10:00:00\"\nelif [ \"$PIPELINE\" = \"wgs\" ]; then\n  QUEUE=\"vlong\"\n  TIME=\"2-00:00:00\"\nfi\n\n# Uppercase version of pipeline\nPIPELINE_UC=${PIPELINE^^}\n\n# where your data and logs live\nWORKDIR=\"/scratch_isilon/projects/0012-hereditary/dbgap/fastq/phs001585/${PIPELINE_UC}/${SAMPLE_ID}\"\n\n# name the generated job script\nJOB_SCRIPT=\"job_${SAMPLE_ID}_${PIPELINE}.slurm\"\n\n# Number of threads\nTHREADS=4\n\n# RAM (x1.5 to help prevent oom-kills)\nMEM=\"24G\"\n\ncat &gt; \"${JOB_SCRIPT}\" &lt;&lt;EOF\n#!/bin/bash\n#SBATCH --job-name=cbicall\n#SBATCH -q ${QUEUE}\n#SBATCH -D ${WORKDIR}\n#SBATCH -e ${WORKDIR}/slurm-%N.%j.err\n#SBATCH -o ${WORKDIR}/slurm-%N.%j.out\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=${THREADS}\n#SBATCH --mem=${MEM}\n#SBATCH -t ${TIME}\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=manuel.rueda@cnag.eu\n\n# Load Python + modules\nmodule load Python/3.10.8-GCCcore-12.2.0\nexport PYTHONPATH=\"/software/biomed/cbi_py3/lib/python3.10/site-packages:${PYTHONPATH}\"\n\n# Set CBICALL exe\nCBICALL_DIR=\"/software/biomed/cbicall\"\nCBICALL=\"\\$CBICALL_DIR/bin/cbicall\"\n\ncd \\$SLURM_SUBMIT_DIR\n\n# write a pipeline\u2010specific yaml\nYAML_FILE=\"${SAMPLE_ID}_${PIPELINE}_param.yaml\"\ncat &lt;&lt;YAML &gt; \"\\${YAML_FILE}\"\nmode: single\npipeline: ${PIPELINE}\nworkflow_engine: bash\ngatk_version: gatk-4.6\nsample: ${WORKDIR}\nprojectdir: ${SAMPLE_ID}_cbicall\ncleanup_bam: false\nYAML\n\nsrun \"\\$CBICALL\" \\\\\n     -p \"\\$YAML_FILE\" \\\\\n     -t $THREADS \\\\\n     --no-color \\\\\n     --no-emoji\nEOF\n\n# submit it\nsbatch \"${JOB_SCRIPT}\"\n</code></pre> How do I cite CBIcall? <p>You can cite the CBIcall paper. Thx!</p> <p>Citation</p> <p>CBIcall: a configuration-driven framework for variant calling in large DNA-seq cohorts. Manuscript In preparation.</p>"},{"location":"help/faq/#last-change-2025-10-15-by-manuel-rueda_4","title":"last change 2025-10-15 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-10-15-by-manuel-rueda_5","title":"last change 2025-10-15 by Manuel Rueda","text":""},{"location":"help/naming-conventions/","title":"\ud83c\udff7\ufe0f Naming Conventions","text":""},{"location":"help/naming-conventions/#scope-and-tool-compatibility","title":"Scope and Tool Compatibility","text":"<p>This document defines naming conventions required to run analyses in <code>cohort</code> mode with legacy tools:</p> <ul> <li>GATK 3.5</li> <li>MTOOLBox</li> </ul> <p>Legacy tools only</p> <p>The directory and subdirectory naming conventions described in this document are mandatory for GATK 3.5 and MTOOLBox. They are not required for GATK \u2265 4.6.</p> <p>With GATK 4.6, sample directory names are ignored by the pipeline and may be any arbitrary string (e.g. <code>NEUMA640ZBR</code>).</p>"},{"location":"help/naming-conventions/#directory-naming","title":"Directory Naming","text":""},{"location":"help/naming-conventions/#format","title":"Format","text":"<pre><code>[ProjectCode]_[SampleType]\n</code></pre> <ul> <li> <p><code>ProjectCode</code>   Exactly 7 characters: <code>[a-zA-Z0-9]</code>   Example: <code>CN99999</code></p> </li> <li> <p><code>SampleType</code>   One of:</p> <ul> <li><code>exome</code></li> <li><code>wgs</code></li> </ul> </li> </ul>"},{"location":"help/naming-conventions/#examples","title":"Examples","text":"<pre><code>CN99999_exome\nCN99999_wgs\n</code></pre>"},{"location":"help/naming-conventions/#subdirectory-naming","title":"Subdirectory Naming","text":""},{"location":"help/naming-conventions/#format_1","title":"Format","text":"<pre><code>[ProjectCode][SampleID][Role]_[SampleTypeShort]\n</code></pre> <ul> <li><code>ProjectCode</code> \u2014 7 characters (<code>[a-zA-Z0-9]</code>)</li> <li><code>SampleID</code> \u2014 2 characters (e.g. <code>01</code>)</li> <li><code>Role</code> \u2014 1 character (<code>P</code>, <code>F</code>, <code>M</code>)</li> <li><code>SampleTypeShort</code> \u2014 must be <code>ex</code></li> </ul>"},{"location":"help/naming-conventions/#example","title":"Example","text":"<pre><code>CN9999901F_ex\n</code></pre>"},{"location":"help/naming-conventions/#fastq-naming-convention","title":"FASTQ Naming Convention","text":"<p>This FASTQ naming convention applies to all tools, including GATK 3.5 and GATK 4.6.</p> <p>It is based on the Illumina specification: support.illumina.com/help/BaseSpace_Sequence_Hub_OLH_009008_2/Source/Informatics/BS/NamingConvention_FASTQ-files-swBS.htm</p>"},{"location":"help/naming-conventions/#sequencing-type-suffix","title":"Sequencing-type suffix","text":"<ul> <li><code>_ex</code> \u2014 exome sequencing</li> <li><code>_wg</code> \u2014 whole genome sequencing</li> </ul>"},{"location":"help/naming-conventions/#example_1","title":"Example","text":"<pre><code>CN9999901P_ex_S5_L001_R1_001.fastq.gz\n</code></pre>"},{"location":"help/naming-conventions/#expected-directory-layout","title":"Expected Directory Layout","text":"<p>Required for cohort mode (legacy)</p> <p>This directory layout is required when running GATK 3.5 or MTOOLBox in <code>cohort</code> mode.</p> <pre><code>CN99999_exome/\n\u2514\u2500\u2500 CN9999901P_ex/\n    \u251c\u2500\u2500 CN9999901P_ex_S1_L001_R1_001.fastq.gz\n    \u2514\u2500\u2500 CN9999901P_ex_S1_L001_R2_001.fastq.gz\n</code></pre>"},{"location":"help/performance/","title":"\u2699\ufe0f Performance","text":""},{"location":"help/performance/#runtime-behavior","title":"Runtime behavior","text":"<p>CBIcall itself uses very little memory for orchestration. The Python wrapper typically remains below 2% of a 16 GB system. Most memory and CPU usage comes from external tools:</p> <ul> <li> <p>BWA-MEM   Memory usage increases with thread count and reference size.   BWA does not provide an internal memory cap, so limiting RAM requires external mechanisms such as <code>ulimit</code>.</p> </li> <li> <p>GATK and Picard   These tools default to using 8 GB of memory.   This value can be adjusted through the CBIcall configuration file.</p> </li> </ul>"},{"location":"help/performance/#parallelization","title":"Parallelization","text":"<p>Parallel execution is supported, but performance does not scale linearly with additional threads. In practice, optimal throughput is usually achieved with 4-6 threads per task.</p> <p>For example, on a 12-core workstation:</p> <ul> <li>Running 3 tasks with 4 threads each is typically preferred than</li> <li>Running a single task with all 12 threads</li> </ul> <p>A runtime example is shown in the figure below.</p> <p>Performance WES</p> <p></p>"},{"location":"help/troubleshooting/","title":"Common errors and troubleshooting","text":"<p>Expected de novo rates in trios</p> <p>For trio analyses, approximate de novo variant rates are:</p> <ul> <li>Probands: ~1%</li> <li>Parents: ~10%</li> </ul> <p>Large deviations from these ranges may indicate technical or pipeline issues that warrant investigation.</p>"},{"location":"help/troubleshooting/#gatk-and-picard-errors","title":"GATK and Picard errors","text":"<p>(wes_single.sh or wes_cohort.sh)</p>"},{"location":"help/troubleshooting/#nan-lod-value-assigned-during-recalibration","title":"NaN LOD value assigned during recalibration","text":"<p>Error message example</p> <p><code>NaN LOD value assigned</code> during VariantRecalibrator or ApplyVQSR.</p> <p>Cause</p> <p>This typically occurs when there are too few INDEL variants (for example, fewer than about 8000) to train a robust negative model. The default minimum INDEL count threshold is 8000 in the VQSR step.</p> <p>Solution</p> <p>Increase the minimum INDEL count threshold in the relevant pipeline script so that VQSR is skipped for samples with low INDEL counts. Only rerun the affected samples. This prevents VariantRecalibrator from trying to build a model on too few variants.</p>"},{"location":"help/troubleshooting/#not-enough-columns-in-dbsnp-line","title":"Not enough columns in dbSNP line","text":"<p>Error message example</p> <p><code>there aren't enough columns for line ... dbsnp_137.hg19.vcf</code></p> <p>Cause</p> <p>One or more lines in the dbSNP VCF file do not conform to the expected VCF column structure (for example, a truncated or malformed record).</p> <p>Solution</p> <ul> <li>Identify the problematic line in the dbSNP VCF.</li> <li>Remove or fix that line.</li> <li>Document the change in a local README or change log so the modification is traceable.</li> </ul>"},{"location":"help/troubleshooting/#error-parsing-text-sam-file","title":"Error parsing text SAM file","text":"<p>Error message example</p> <p><code>Error parsing text SAM file. Not enough fields; File /dev/stdin; Line 105120626...</code></p> <p>Cause</p> <p>Some SRA or dbGaP datasets include duplicate or problematic reads. When piping BWA output directly into AddOrReplaceReadGroups, secondary and supplementary alignments can cause issues and lead to collisions or invalid lines as seen by Picard or GATK.</p> <p>Solution</p> <p>Remove secondary (0x100) and supplementary (0x800) alignments from the BWA stream before adding read groups.</p> <p>In <code>wes_single.sh</code>, uncomment the filtering step in the alignment pipe, for example:</p> <pre><code>bwa mem -M -t \"$THREADS\" \"$REFGZ\" \"$R1\" \"$R2\"   | samtools view -bSh -F 0x900 -   | gatk AddOrReplaceReadGroups ...\n</code></pre> <p>This filtering prevents problematic alignments from reaching Picard or GATK and avoids the parsing error.</p>"},{"location":"help/troubleshooting/#mtoolbox-errors-and-mtdna-specific-issues","title":"MToolBox errors and mtDNA specific issues","text":""},{"location":"help/troubleshooting/#unsupported-n-cigar-operations","title":"Unsupported N CIGAR operations","text":"<p>Symptom</p> <p>MToolBox fails with an error related to unsupported N operations in CIGAR strings.</p> <p>Solution</p> <p>Add the <code>--filter_reads_with_N_cigar</code> flag in <code>MToolBox.sh</code> (around the main <code>bwa mem</code> or SAM tools invocations, typically near line 386 in your local copy).</p> <p>This discards reads with N operations in the CIGAR string before downstream processing, avoiding MToolBox failures.</p>"},{"location":"help/troubleshooting/#low-coverage-and-unreliable-heteroplasmy-fractions","title":"Low coverage and unreliable heteroplasmy fractions","text":"<p>Symptom</p> <ul> <li>Very low coverage mtDNA samples.</li> <li>Heteroplasmic fraction (HF) estimates appear noisy or unreliable.</li> </ul> <p>Guideline</p> <ul> <li>Below about 10x mtDNA coverage, HF estimates become unreliable and may be biologically meaningless.</li> <li>At higher coverage, HF values tend to be robust, even when coverage varies across samples.</li> </ul> <p>Recommended actions</p> <ul> <li>Flag samples with less than 10x median mtDNA coverage for review.</li> <li>Interpret HF with caution in low coverage samples, or exclude them from HF based analyses.</li> <li>Consider resequencing or deeper coverage if mtDNA heteroplasmy is critical to the study.</li> </ul>"},{"location":"installation/docker/","title":"Containerized Installation","text":""},{"location":"installation/docker/#downloading-required-databases-and-software","title":"Downloading Required Databases and Software","text":"<p>Note: this process can be lenghty.</p> <p>Begin by downloading the required databases and software. Save the data outside the container; this preserves it across container restarts and lets you update the software without downloading the data again.</p> <p>Install dependencies for Python 3:</p> <pre><code>pip3 install gdown\n</code></pre> <p>Finally, navigate to a directory where you want the databases stored and execute:</p> <pre><code>wget https://raw.githubusercontent.com/mrueda/cbicall/refs/heads/main/scripts/01_download_external_data.py\npython3 ./01_download_external_data.py\n</code></pre> <p>Note: Google Drive can be a tad restrictive with the download. If you get an error, please use the error URL link in a browser and you should be able to retrieve it there.</p> <p>The files are located at: GDrive Link</p> <p>Once downloaded, perform a checksum to make sure the files were not corrupted:</p> <pre><code>md5sum -c data.tar.gz.md5\n</code></pre> <p>Now let's reassemble the split files into the original tar archive:</p> <pre><code>cat data.tar.gz.part-?? &gt; data.tar.gz\n</code></pre> <p>Clean up split files to save space (when you think you are ready!):</p> <pre><code>rm data.tar.gz.part-??\n</code></pre> <p>Extract the tar archive:</p> <pre><code>tar -xzvf data.tar.gz\n</code></pre> <p>Finally, in the <code>cbicall</code> repo:</p> <p>Change <code>DATADIR</code> variable in <code>workflows/bash/*/parameters.sh</code> and <code>workflows/snakemake/*/config.yaml</code> so that it matches the location of your downloaded data.</p>"},{"location":"installation/docker/#method-1-installing-from-docker-hub-fast","title":"Method 1: Installing from Docker Hub (fast)","text":"<p>Pull the latest Docker image from Docker Hub:</p> <pre><code>docker pull manuelrueda/cbicall:latest\ndocker image tag manuelrueda/cbicall:latest cnag/cbicall:latest\n</code></pre>"},{"location":"installation/docker/#method-2-installing-from-dockerfile-slow","title":"Method 2: Installing from Dockerfile (slow)","text":"<p>Download the <code>Dockerfile</code> from GitHub:</p> <pre><code>wget https://raw.githubusercontent.com/CNAG-Biomedical-Informatics/cbicall/main/docker/Dockerfile\n</code></pre> <p>Then build the container:</p> <ul> <li>For Docker version 19.03 and above (supports buildx):</li> </ul> <pre><code>docker buildx build --no-cache -t cnag/cbicall:latest .\n</code></pre> <ul> <li>For Docker versions older than 19.03 (no buildx support):</li> </ul> <pre><code>docker build --no-cache -t cnag/cbicall:latest .\n</code></pre>"},{"location":"installation/docker/#running-and-interacting-with-the-container","title":"Running and Interacting with the Container","text":"<pre><code># Please update '/absolute/path/to/cbicall-data' with your actual local data path\n#docker run -tid --volume /absolute/path/to/cbicall-data:/cbicall-data -e USERNAME=root --name cbicall cnag/cbicall:latest\n\n# Real example\n#docker run -tid --volume /media/mrueda/4TBB/cbicall-data:/cbicall-data -e USERNAME=root --name cbicall cnag/cbicall:latest\n</code></pre> <p>To connect to the container:</p> <pre><code>docker exec -ti cbicall bash\n</code></pre> <p>Finally, inside the <code>cbicall</code> repo:</p> <p>Change <code>DATADIR</code> variable in <code>workflows/bash/gatk-4.6/parameters.sh</code> and <code>workflows/snakemake/gatk-4.6/config.yaml</code> to <code>/cbicall-data</code>.</p>"},{"location":"installation/docker/#performing-integration-tests","title":"Performing integration tests","text":"<p>Inside the container:</p> <p>WES</p> <pre><code>cd examples/input\n./run_tests.sh --wes\n</code></pre> <p>mtDNA</p> <pre><code>cd examples/input\n./run_tests.sh --mit\n</code></pre>"},{"location":"installation/docker/#system-requirements","title":"System requirements","text":"<ul> <li>OS/ARCH supported: linux/amd64 and linux/arm64.</li> <li>Ideally a Debian-based distribution (Ubuntu or Mint), but any other (e.g., CentOS, OpenSUSE) should do as well (untested).</li> <li>16GB of RAM</li> <li>&gt;= 1 core (ideally i7 or Xeon).</li> <li>At least 100GB HDD.</li> </ul>"},{"location":"installation/docker/#common-errors-symptoms-and-treatment","title":"Common errors: Symptoms and treatment","text":"<ul> <li>Dockerfile:<pre><code>  * DNS errors\n\n    - Error: Temporary failure resolving 'foo'\n\n      Solution: https://askubuntu.com/questions/91543/apt-get-update-fails-to-fetch-files-temporary-failure-resolving-error\n</code></pre> </li> </ul>"},{"location":"installation/non-containerized/","title":"Non-containerized installation","text":"<p>Feel free to work with your preferred virtual environment. For this document, we'll move directly to the setup steps.</p>"},{"location":"installation/non-containerized/#method-1-download-from-github","title":"Method 1: Download from GitHub","text":"<p>Use <code>git clone</code> to get the latest (stable) version:</p> <pre><code>git clone https://github.com/CNAG-Biomedical-Informatics/cbicall.git\ncd cbicall\n</code></pre> <p>If you only new to update to the lastest version do:</p> <pre><code>git pull\n</code></pre> <p>Install dependencies for Python 3:</p> <pre><code>pip3 install -r requirements.txt\n</code></pre> <p>Note: If you are installing <code>cbicall</code> in an HPC environment for shared use, we recommend installing the required Python 3 modules in a central location. This allows users to simply do:</p> <pre><code># Load Python + modules\nmodule load Python/3.10.8-GCCcore-12.2.0\nexport PYTHONPATH=\"/software/biomed/cbi_py3/lib/python3.10/site-packages:${PYTHONPATH}\"\n</code></pre> <p>Testing the deployment:</p> <pre><code>pytest\n</code></pre>"},{"location":"installation/non-containerized/#downloading-required-databases-and-software","title":"Downloading Required Databases and Software","text":"<p>Note: this process can be lenghty.</p> <p>Navigate to a directory where you want the databases stored and execute:</p> <pre><code>python3 $path_to_cbicall/scripts/01_download_external_data.py  # Replace $path_to_cbicall with your CBICall installation path.\n</code></pre> <p>Note: Google Drive can be a tad restrictive with the download. If you get an error, please use the error URL link in a browser and you should be able to retrieve it there.</p> <p>The files are located at: GDrive Link</p> <p>Once downloaded, perform a checksum to make sure the files were not corrupted:</p> <pre><code>md5sum -c data.tar.gz.md5\n</code></pre> <p>Now let's reassemble the split files into the original tar archive:</p> <pre><code>cat data.tar.gz.part-?? &gt; data.tar.gz\n</code></pre> <p>Clean up split files to save space (when you think you are ready!):</p> <pre><code>rm data.tar.gz.part-??\n</code></pre> <p>Extract the tar archive:</p> <pre><code>tar -xzvf data.tar.gz\n</code></pre> <p>Finally, in the <code>cbicall</code> repo:</p> <p>Change <code>DATADIR</code> variable in <code>workflows/bash/*/parameters.sh</code> and <code>workflows/snakemake/*/config.yaml</code> so that it matches the location of your downloaded data.</p> <p>Ok, finally we are going to install <code>Java 8</code> in case you don't have it already:</p> <pre><code>sudo apt install openjdk-8-jdk # In some systems you might need Java 17 -&gt; openjdk-17-jre\n</code></pre>"},{"location":"installation/non-containerized/#performing-integration-tests","title":"Performing integration tests","text":"<p>Once you are in the root directory of the repo:</p> <p>WES:</p> <pre><code>cd examples/input\n./run_tests.sh --wes\n</code></pre> <p>mtDNA:</p> <pre><code>cd examples/input\n./run_tests.sh --mit\n</code></pre>"},{"location":"installation/non-containerized/#system-requirements","title":"System requirements","text":"<ul> <li>OS/ARCH supported: linux/amd64 and linux/arm64.</li> <li>Ideally a Debian-based distribution (Ubuntu or Mint), but any other (e.g., CentOS, OpenSUSE) should do as well (untested).</li> <li>Python &gt;= 3.8</li> <li>Java 8</li> <li>16GB of RAM</li> <li>&gt;= 1 core (ideally i7 or Xeon).</li> <li>At least 100GB HDD.</li> </ul>"},{"location":"installation/non-containerized/#platform-compatibility","title":"Platform Compatibility","text":"<p>This distribution is written in Python 3 and is intended to run on any platform supported by Python 3. It has been tested on Debian Linux and macOS. Please report any issues.</p>"},{"location":"pipelines/mtdna/","title":"Overview","text":"<p>These pipelines extract mitochondrial reads from exome data and run MToolBox to generate mtDNA variant calls, annotations, and heteroplasmy estimates.</p> <p>There are two processing modes:</p> <ul> <li>Single-sample analysis (<code>mit_single</code>)</li> <li>Cohort / family analysis (<code>mit_cohort</code>)</li> </ul> <p>Both consume WES single-sample outputs and assume this nomenclature.</p>"},{"location":"pipelines/mtdna/#choosing-a-pipeline","title":"Choosing a Pipeline","text":"Use Case Pipeline Description Analyze one individual <code>mit_single</code> Fast, sample-specific mtDNA variant calling + HF/DP/GT extraction Analyze a full family or cohort <code>mit_cohort</code> Joint variant calling across samples; useful for transmission and segregation checks"},{"location":"pipelines/mtdna/#mtdna-pipelines","title":"mtDNA Pipelines","text":"Single-Sample (<code>mit_single</code>)Cohort (<code>mit_cohort</code>)"},{"location":"pipelines/mtdna/#mtdna-single-sample-pipeline","title":"mtDNA Single-Sample Pipeline","text":"See Bash pipeline <pre><code>#!/usr/bin/env bash\n# \n#   mtDNA Pipeline Bash script.\n#   This pipeline works at the the sample level, for cohorts you will \n#   need to excute \"mit_cohort.sh\". This way, if a new relatives comes, \n#   you cand easily add it a posteriori.\n#\n#   Last Modified; March/05/2025\n#\n#   $VERSION taken from CBICall\n#\n#   Copyright (C) 2025 Manuel Rueda - CNAG (manuel.rueda@cnag.eu)\n\nset -eu\n\nfunction usage {\n\n    USAGE=\"\"\"\n    Usage: $0 -t n_threads\n\n    NB1: The script is expecting that you follow SRTI nomenclature for samples\n\nMA00047_exome\n\u2514\u2500\u2500 MA0004701P_ex  &lt;--- ID taken from here\n    \u251c\u2500\u2500 MA0004701P_ex_S5_L001_R1_001.fastq.gz\n    \u251c\u2500\u2500 MA0004701P_ex_S5_L001_R2_001.fastq.gz\n    \u251c\u2500\u2500 MA0004701P_ex_S5_L002_R1_001.fastq.gz\n    \u251c\u2500\u2500 MA0004701P_ex_S5_L002_R2_001.fastq.gz\n    \u2514\u2500\u2500 cbicall_bash_mit_single_gatk-3.5_* &lt;- The script expects that you are submitting the job from inside this directory\n    \"\"\"\n    echo \"$USAGE\"\n    exit 1\n}\n\n\n# Check arguments\nif [ $# -eq 0 ]\n then\n  usage\nfi\n\n# parsing Arguments\nkey=\"$1\"\ncase $key in\n    -t|--t)\n    THREADS=\"$2\"\nesac\n\n# Determine the directory where the script resides\nBINDIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\n\n# Source parameters.sh from the same directory\nsource \"$BINDIR/parameters.sh\"\n\n# Check ARCH\nif [ \"$ARCH\" == \"aarch64\" ]\n then\n  echo \"mit_single cannot be performed with: $ARCH\"\n  exit 1\nfi\n\n# Set up variables and Defining directories\nDIR=$( pwd )\nBINDIRMTB=$BINDIR/../../../mtdna\nPYBINDIR=$BINDIR/../../../browser\nASSETS=$PYBINDIR/assets\n\nid=$( echo \"$DIR\" | awk -F'/' '{print $(NF-1)}' | awk -F'_' '{print $1}' )\n# The mtb_id needs to have this format LP6005831-???_???.bam, otherwise MToolBox will fail\nmtb_id=\"$id-DNA_MIT\"\njob_id=$( echo \"$DIR\" | awk -F'_' '{print $NF}' )\n\n# Set up dirs\nVARCALLDIR=$DIR/01_mtoolbox\nBROWSERDIR=$DIR/02_browser\nmkdir \"$VARCALLDIR\"\nmkdir $BROWSERDIR\n\n# From now on we will work on VARCALL dir\ncd \"$VARCALLDIR\"\n\n# Using Samtools to extract chrM\necho \"Extracting Mitochondrial DNA from exome BAM file...\"\n\nout_raw=$mtb_id.bam\n\nbam_raw=\"\"\n\np35='../../*cbicall_bash_wes_single_*gatk-3.5*/01_bam/input.merged.filtered.realigned.fixed.bam'\nlist35=$(ls -1 $p35 2&gt;/dev/null | grep -v 'ref_cbicall' || true)\nn35=$(printf \"%s\\n\" \"$list35\" | sed '/^$/d' | wc -l)\n\nif [ \"$n35\" -gt 1 ]; then\n  echo \"ERROR: More than one GATK 3.5 BAM found (excluding ref_cbicall):\" &gt;&amp;2\n  printf \"%s\\n\" \"$list35\" &gt;&amp;2\n  exit 1\nelif [ \"$n35\" -eq 1 ]; then\n  bam_raw=$(printf \"%s\\n\" \"$list35\" | head -n 1)\n  echo \"Using GATK 3.5 BAM: $bam_raw\"\nfi\n\nif [ -z \"$bam_raw\" ]; then\n  p46=\"../../*cbicall_bash_w[ge]s_single_*gatk-4.6*/01_bam/${id}.rg.merged.dedup.recal.bam\"\n  list46=$(ls -1 $p46 2&gt;/dev/null | grep -v 'ref_cbicall' || true)\n  n46=$(printf \"%s\\n\" \"$list46\" | sed '/^$/d' | wc -l)\n\n  if [ \"$n46\" -gt 1 ]; then\n    echo \"ERROR: More than one GATK 4.6 BAM found (excluding ref_cbicall):\" &gt;&amp;2\n    printf \"%s\\n\" \"$list46\" &gt;&amp;2\n    exit 1\n  elif [ \"$n46\" -eq 1 ]; then\n    bam_raw=$(printf \"%s\\n\" \"$list46\" | head -n 1)\n    echo \"Using GATK 4.6 BAM: $bam_raw\"\n  fi\nfi\n\nif [ -z \"$bam_raw\" ]; then\n  echo \"ERROR: Could not find BAM for ID '$id' (excluding ref_cbicall) in either:\" &gt;&amp;2\n  echo \"  $p35\" &gt;&amp;2\n  echo \"  $p46\" &gt;&amp;2\n  exit 1\nfi\n\nBAMDIR=$(dirname \"$bam_raw\")\nbam_raw_index=\"${bam_raw%.bam}.bai\"\n\nif [[ $REF == *b37*.fasta ]]\n then\n  chrM=MT\n else\n  chrM=chrM\nfi\n\n$SAM view -b $bam_raw $chrM &gt; $out_raw\n$SAM index $out_raw\n\n# Performing Variant calling and annotation with MToolBox\necho \"Analyzing mitochondrial DNA with MToolBox...\"\n\n(\n  export PATH=\"$MTOOLBOXDIR:$PATH\"\n  export PYTHONNOUSERSITE=1\n\n  echo \"Using numpy and pandas versions:\"\n\n  # --- First choice: portable prefix python2 (non-cluster) ---\n  export PATH=\"$PY27_PREFIX/bin:$PATH\"\n  export PYTHONHOME=\"$PY27_PREFIX\"\n  export PYTHONPATH=\"$PY27_PREFIX/lib/python2.7/site-packages\"\n\n  if python2 -c \"import numpy, pandas\" &gt;/dev/null 2&gt;&amp;1; then\n    python2 -c \"import numpy, pandas; print(numpy.__version__, pandas.__version__)\"\n  else\n    echo \"Portable python2/numpy failed; trying cluster module Python2...\"\n\n    # --- Fallback: cluster module python2 ---\n    unset PYTHONHOME\n    module purge\n    module load \"${PY27_MODULE:-Python/2.7.18-GCCcore-11.2.0}\"\n\n    # Use the shipped site-packages path (same one you copy around)\n    export PYTHONPATH=\"$PY27_PREFIX/lib/python2.7/site-packages${PYTHONPATH:+:$PYTHONPATH}\"\n\n    python2 -c \"import numpy, pandas; print(numpy.__version__, pandas.__version__)\" || {\n      echo \"ERROR: numpy/pandas not importable with either portable prefix or module python2\" &gt;&amp;2\n      python2 -c \"import sys; print(sys.executable); print('\\n'.join(sys.path))\" &gt;&amp;2 || true\n      exit 1\n    }\n  fi\n\n  MToolBox.sh -i \"$BINDIRMTB/MToolBox_config.sh\" -m \"-t $THREADS\"\n)\n\n# We will be using the file 'prioritized_variants.txt'\n# Getting GT/ DP and HF information rom VCF_file.vcf\n# HF information is also in file(s) OUT*/*annotation.csv\n# OUT* may contain &gt; 1 *annotation (haplotypes), still the HF will be the same on each\n\n# We will append the columns at the end\necho \"Appending Heteroplasmic Fraction to the output...\"\nvcf_file=\"VCF_file.vcf\"\n\n# Check if the file exists\nif [ ! -f \"$vcf_file\" ]; then\n    echo \"Error: File '$vcf_file' not found!\"\n    exit 1\nfi\n\nin_file=prioritized_variants.txt\nout_file=append_$$.txt\nfinal_file=mit_prioritized_variants.txt\nparse_var=$BINDIR/parse_var.pl\necho -e \"REF\\tALT\\tGT\\tDP\\tHF\" &gt; $out_file\nfor var in $(cut -f1 $in_file | sed '1d' | $parse_var) \ndo\n   grep -P \"chrMT\\t$var\\t\" $vcf_file | cut -f4,5,10 | tr ':' '\\t' |cut -f1-5 &gt;&gt;  $out_file || true\ndone\npaste $in_file $out_file &gt; $final_file\nrm $out_file\n\n# HMTL creation\necho \"Creating Browser HTML...\"\nmit_json=mit.json\nmit_raw_json=mit.raw.json\n$PYBINDIR/mtb2json.py  -i $final_file -f json &gt; $mit_raw_json\n$PYBINDIR/mtb2json.py  -i $final_file -f json4html &gt; $BROWSERDIR/$mit_json\n$PYBINDIR/mtb2html.py --id $id --json $mit_json --out $BROWSERDIR/$job_id.html --job-id $job_id\nln -s $ASSETS $BROWSERDIR/assets\n\ncat&lt;&lt;EOF&gt;$BROWSERDIR/README.txt\n# To visualize &lt;$job_id.html&gt;:\n\n# Option 1: Open &lt;176099009134887.html&gt; directly in Chromium\nchromium --allow-file-access-from-files --disable-web-security $job_id.html\n\n# Option 2: Use an HTTP server. Example using Python 3:\npython3 -m http.server\nEOF\n\n# Fin\necho \"All done!!!\"\nexit\n</code></pre>"},{"location":"pipelines/mtdna/#workflow-diagram","title":"Workflow Diagram","text":"View diagram flowchart TD     A[\"Input WES BAM from single sample pipeline\"]     B[\"Extract mitochondrial reads (chrM/MT)\"]     C[\"Create mtDNA-only BAM\"]     D[\"Run MToolBox (RSRS reference)\"]     E[\"Get VCF + prioritized variants\"]     F[\"Extract GT/DP/HF for each variant\"]     G[\"Write mit_prioritized_variants.txt\"]     A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G"},{"location":"pipelines/mtdna/#summary","title":"Summary","text":"<p>The <code>mit_single</code> pipeline processes one individual at a time. It extracts mtDNA reads, runs MToolBox, and enriches the prioritized variants with:</p> <ul> <li>GT \u2014 genotype  </li> <li>DP \u2014 depth  </li> <li>HF \u2014 heteroplasmic fraction  </li> </ul>"},{"location":"pipelines/mtdna/#inputs","title":"Inputs","text":"<ul> <li>Run inside directory: <code>cbicall_bash_mit_single_*</code></li> <li>Needs WES single-sample directory:   <code>../../cbicall_bash_wes_single*/01_bam/input.merged.filtered.realigned.fixed.bam</code></li> <li><code>parameters.sh</code> provides:</li> <li><code>REF</code></li> <li><code>SAM</code> path</li> <li><code>MTOOLBOXDIR</code></li> </ul>"},{"location":"pipelines/mtdna/#outputs","title":"Outputs","text":"File Description <code>VCF_file.vcf</code> mtDNA VCF from MToolBox <code>prioritized_variants.txt</code> Raw prioritized list <code>mit_prioritized_variants.txt</code> Final prioritized list with GT/DP/HF"},{"location":"pipelines/mtdna/#mtdna-cohort-pipeline","title":"mtDNA Cohort Pipeline","text":"See Bash pipeline <pre><code>#!/usr/bin/env bash\n#\n#   mtDNA Pipeline Cohort Bash script.\n#\n#   Last Modified; Dec/29/2025\n#\n#   $VERSION taken from CBICall\n#\n#   Copyright (C) 2025 Manuel Rueda - CNAG (manuel.rueda@cnag.eu)\n\nset -eu\n\nfunction usage {\n\n    USAGE=\"\"\"\n    Usage: $0 -t n_threads\n\n    NB1: The script is expecting that you follow SRTI nomenclature for samples\n    NB2: There is no need to run wes_cohort prior to mit_cohort.\n\nMA00024_exome  &lt;-- ID taken from here\n\u251c\u2500\u2500 MA0002401P_ex\n\u2502   \u2514\u2500\u2500 cbicall_*_wes_single_*gatk-*/...\n\u251c\u2500\u2500 MA0002402M_ex\n\u2502   \u2514\u2500\u2500 cbicall_*_wes_single_*gatk-*/...\n\u2514\u2500\u2500 cbicall_bash_mit_cohort_* &lt;- Submit from inside this directory\n    \"\"\"\n    echo \"$USAGE\"\n    exit 1\n}\n\n# Check arguments\nif [ $# -eq 0 ]; then\n  usage\nfi\n\n# parsing Arguments\nkey=\"$1\"\ncase $key in\n    -t|--t)\n    THREADS=\"${2:-}\"\n    ;;\n    *)\n    usage\n    ;;\nesac\n\nif [ -z \"${THREADS:-}\" ]; then\n  usage\nfi\n\n# Determine the directory where the script resides\nBINDIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\n\n# Source parameters.sh from the same directory\nsource \"$BINDIR/parameters.sh\"\n\n# Check ARCH (same behavior as mit_single)\nif [ \"${ARCH:-}\" = \"aarch64\" ]; then\n  echo \"mit_cohort cannot be performed with: ${ARCH:-aarch64}\"\n  exit 1\nfi\n\n# Set up variables and Defining directories\nDIR=\"$(pwd)\"\n\n# Check that nomenclature exists\nif [[ \"$DIR\" != *cbicall_bash_mit_cohort* ]]; then\n  usage\nfi\n\n# Anchor project-relative directories (same as mit_single)\nBINDIRMTB=\"$BINDIR/../../../mtdna\"\nPYBINDIR=\"$BINDIR/../../../browser\"\nASSETS=\"$PYBINDIR/assets\"\n\n# cohort id (format: &lt;PROJECT&gt;-DNA_MIT)\ncohort=\"$(echo \"$DIR\" | awk -F'/' '{print $(NF-1)}' | awk -F'_' '{print $1}' | sed 's/$/-DNA_MIT/')\"\necho \"$cohort\"\n\njob_id=\"$(echo \"$DIR\" | awk -F'_' '{print $NF}')\"\n\n# Working dirs\nVARCALLDIR=\"$DIR/01_mtoolbox\"\nBROWSERDIR=\"$DIR/02_browser\"\n\nmkdir -p \"$VARCALLDIR\"\nmkdir -p \"$BROWSERDIR\"\n\ncd \"$VARCALLDIR\"\n\n# ------------------------------------------------------------\n# Extract chrM from each sample BAM found in sibling sample dirs\n# ------------------------------------------------------------\n\necho \"Extracting Mitochondrial DNA from exome BAM files...\"\n\n# Determine chrM naming based on REF\nif [[ \"$REF\" == *b37*.fasta* ]]; then\n  chrM=\"MT\"\nelse\n  chrM=\"chrM\"\nfi\n\n# Find candidate sample directories one level up (same layout as your tree)\n# Example: ../MA0002401P_ex/...\nsample_dirs=$(ls -d ../../??????????_ex 2&gt;/dev/null || true)\n\nif [ -z \"${sample_dirs:-}\" ]; then\n  echo \"ERROR: No sample directories matching ../??????????_ex were found.\" &gt;&amp;2\n  exit 1\nfi\n\nfound_any=0\n\nfor sdir in $sample_dirs; do\n  # Sample ID (directory name prefix before first '_')\n  sid=\"$(basename \"$sdir\" | awk -F'_' '{print $1}')\"\n  mtb_id=\"${sid}-DNA_MIT\"\n\n  bam_raw=\"\"\n\n  # --- Prefer GATK 3.5 bam layout (wes_single, fixed.bam) ---\n  p35=\"$sdir/\"*cbicall_bash_wes_single_*gatk-3.5*/01_bam/input.merged.filtered.realigned.fixed.bam\n  list35=$(ls -1 $p35 2&gt;/dev/null | grep -v 'ref_cbicall' || true)\n  n35=$(printf \"%s\\n\" \"$list35\" | sed '/^$/d' | wc -l)\n\n  if [ \"$n35\" -gt 1 ]; then\n    echo \"ERROR: More than one GATK 3.5 BAM found for sample '$sid' (excluding ref_cbicall):\" &gt;&amp;2\n    printf \"%s\\n\" \"$list35\" &gt;&amp;2\n    exit 1\n  elif [ \"$n35\" -eq 1 ]; then\n    bam_raw=$(printf \"%s\\n\" \"$list35\" | head -n 1)\n    echo \"Using GATK 3.5 BAM for $sid: $bam_raw\"\n  fi\n\n  # --- Otherwise try GATK 4.6 bam layout (wes/wgs single, recal.bam) ---\n  if [ -z \"$bam_raw\" ]; then\n    p46=\"$sdir/\"*cbicall_bash_w[ge]s_single_*gatk-4.6*/01_bam/\"$sid\".rg.merged.dedup.recal.bam\n    list46=$(ls -1 $p46 2&gt;/dev/null | grep -v 'ref_cbicall' || true)\n    n46=$(printf \"%s\\n\" \"$list46\" | sed '/^$/d' | wc -l)\n\n    if [ \"$n46\" -gt 1 ]; then\n      echo \"ERROR: More than one GATK 4.6 BAM found for sample '$sid' (excluding ref_cbicall):\" &gt;&amp;2\n      printf \"%s\\n\" \"$list46\" &gt;&amp;2\n      exit 1\n    elif [ \"$n46\" -eq 1 ]; then\n      bam_raw=$(printf \"%s\\n\" \"$list46\" | head -n 1)\n      echo \"Using GATK 4.6 BAM for $sid: $bam_raw\"\n    fi\n  fi\n\n  if [ -z \"$bam_raw\" ]; then\n    echo \"WARNING: Could not find BAM for sample '$sid' (excluding ref_cbicall). Skipping.\" &gt;&amp;2\n    continue\n  fi\n\n  found_any=1\n\n  # Ensure index exists as *.bam.bai (MToolBox/samtools expectations are happier)\n  bam_raw_index=\"${bam_raw%.bam}.bai\"\n  bam_raw_index_ok=\"${bam_raw}.bai\"\n  if [ -s \"$bam_raw_index\" ] &amp;&amp; [ ! -s \"$bam_raw_index_ok\" ]; then\n    cp \"$bam_raw_index\" \"$bam_raw_index_ok\"\n  fi\n\n  out_raw=\"${mtb_id}.bam\"\n\n  \"$SAM\" view -b \"$bam_raw\" \"$chrM\" &gt; \"$out_raw\"\n  \"$SAM\" index \"$out_raw\"\ndone\n\nif [ \"$found_any\" -ne 1 ]; then\n  echo \"ERROR: No usable sample BAMs found. Nothing to do.\" &gt;&amp;2\n  exit 1\nfi\n\n# ------------------------------------------------------------\n# Run MToolBox (same python2 / numpy,pandas bootstrap as mit_single)\n# ------------------------------------------------------------\n\necho \"Analyzing mitochondrial DNA with MToolBox...\"\n\n(\n  export PATH=\"$MTOOLBOXDIR:$PATH\"\n  export PYTHONNOUSERSITE=1\n\n  echo \"Using numpy and pandas versions:\"\n\n  # --- First choice: portable prefix python2 (non-cluster) ---\n  export PATH=\"$PY27_PREFIX/bin:$PATH\"\n  export PYTHONHOME=\"$PY27_PREFIX\"\n  export PYTHONPATH=\"$PY27_PREFIX/lib/python2.7/site-packages\"\n\n  if python2 -c \"import numpy, pandas\" &gt;/dev/null 2&gt;&amp;1; then\n    python2 -c \"import numpy, pandas; print(numpy.__version__, pandas.__version__)\"\n  else\n    echo \"Portable python2/numpy failed; trying cluster module Python2...\"\n\n    # --- Fallback: cluster module python2 ---\n    unset PYTHONHOME\n    module purge\n    module load \"${PY27_MODULE:-Python/2.7.18-GCCcore-11.2.0}\"\n\n    # Use the shipped site-packages path (same one you copy around)\n    export PYTHONPATH=\"$PY27_PREFIX/lib/python2.7/site-packages${PYTHONPATH:+:$PYTHONPATH}\"\n\n    python2 -c \"import numpy, pandas; print(numpy.__version__, pandas.__version__)\" || {\n      echo \"ERROR: numpy/pandas not importable with either portable prefix or module python2\" &gt;&amp;2\n      python2 -c \"import sys; print(sys.executable); print('\\n'.join(sys.path))\" &gt;&amp;2 || true\n      exit 1\n    }\n  fi\n\n  # Use the same config file path as mit_single (from mtdna folder)\n  MToolBox.sh -i \"$BINDIRMTB/MToolBox_config.sh\" -m \"-t $THREADS\"\n)\n\n# ------------------------------------------------------------\n# Post-process prioritized_variants.txt -&gt; mit_prioritized_variants.txt\n# ------------------------------------------------------------\n\n# We will be using the file 'prioritized_variants.txt'\n# Getting GT/ DP and HF information from VCF_file.vcf\n# Append cohort-wide GT/DP/HF using parse_prioritized.pl\n\necho \"Appending Heteroplasmic Fraction to the output...\"\n\nvcf_file=\"VCF_file.vcf\"\nvcf_tmp=\"VCF_file_$$.vcf\"\nin_file=\"prioritized_variants.txt\"\nout_file=\"append_$$.txt\"\nfinal_file=\"mit_prioritized_variants.txt\"\n\nparse_var=\"$BINDIR/parse_var.pl\"\nparse_prior=\"$BINDIR/parse_prioritized.pl\"\n\n# Check if files exist\nif [ ! -f \"$vcf_file\" ]; then\n    echo \"Error: File '$vcf_file' not found!\"\n    exit 1\nfi\nif [ ! -f \"$in_file\" ]; then\n    echo \"Error: File '$in_file' not found!\"\n    exit 1\nfi\n\n# Keep header\ngrep '^#CHROM' \"$vcf_file\" &gt; \"$vcf_tmp\"\n\n# Add only variants of interest\nfor var in $(cut -f1 \"$in_file\" | sed '1d' | \"$parse_var\")\ndo\n  grep -P \"chrMT\\t$var\\t\" \"$vcf_file\" &gt;&gt; \"$vcf_tmp\"  || echo \"$var not found\"\ndone\n\n# Parse per-sample fields (GT/DP/HF across all samples)\n\"$parse_prior\" -i \"$vcf_tmp\" &gt; \"$out_file\"\n\n# Merge onto prioritized_variants\npaste \"$in_file\" \"$out_file\" &gt; \"$final_file\"\nrm \"$vcf_tmp\" \"$out_file\"\n\n# ------------------------------------------------------------\n# Optional: Browser HTML output (same tooling/paths as mit_single)\n# ------------------------------------------------------------\n\necho \"Creating Browser HTML...\"\nmit_json=\"mit.json\"\nmit_raw_json=\"mit.raw.json\"\n\n\"$PYBINDIR/mtb2json.py\" -i \"$final_file\" -f json &gt; \"$mit_raw_json\"\n\"$PYBINDIR/mtb2json.py\" -i \"$final_file\" -f json4html &gt; \"$BROWSERDIR/$mit_json\"\n\"$PYBINDIR/mtb2html.py\" --id \"$cohort\" --json \"$mit_json\" --out \"$BROWSERDIR/$job_id.html\" --job-id \"$job_id\"\nln -s \"$ASSETS\" \"$BROWSERDIR/assets\" 2&gt;/dev/null || true\n\ncat &lt;&lt;EOF &gt; \"$BROWSERDIR/README.txt\"\n# To visualize &lt;$job_id.html&gt;:\n\n# Option 1: Open &lt;$job_id.html&gt; directly in Chromium\nchromium --allow-file-access-from-files --disable-web-security $job_id.html\n\n# Option 2: Use an HTTP server. Example using Python 3:\npython3 -m http.server\nEOF\n\necho \"All done!!!\"\nexit\n</code></pre>"},{"location":"pipelines/mtdna/#workflow-diagram_1","title":"Workflow Diagram","text":"View diagram flowchart TD     A[\"Input all WES single-sample directories\"]     B[\"Extract mtDNA BAMs for each sample\"]     C[\"mtDNA BAM collection\"]     D[\"Run MToolBox jointly on cohort\"]     E[\"Get cohort VCF + prioritized variants\"]     F[\"Extract GT/DP/HF for all samples\"]     G[\"Write mit_prioritized_variants.txt (cohort)\"]     A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G"},{"location":"pipelines/mtdna/#summary_1","title":"Summary","text":"<p><code>mit_cohort</code> processes all samples together, generating a joint mtDNA variant table useful for:</p> <ul> <li>Family mtDNA transmission studies  </li> <li>Maternal-lineage analysis  </li> <li>Variant segregation and consistency checks  </li> </ul>"},{"location":"pipelines/mtdna/#inputs_1","title":"Inputs","text":"<ul> <li>Run inside: <code>cbicall_bash_mit_cohort_*</code></li> <li>Expects WES single-sample directories:   <code>../../??????????_ex/cbicall_bash_wes_single*/01_bam/</code></li> <li><code>parameters.sh</code> defines <code>REF</code>, <code>SAM</code>, <code>MTOOLBOXDIR</code></li> </ul>"},{"location":"pipelines/mtdna/#outputs_1","title":"Outputs","text":"File Description <code>VCF_file.vcf</code> mtDNA VCF for full cohort <code>prioritized_variants.txt</code> MToolBox-prioritized list <code>mit_prioritized_variants.txt</code> Joint variant table with GT/DP/HF"},{"location":"pipelines/mtdna/#when-to-use-each-pipeline","title":"When to Use Each Pipeline","text":""},{"location":"pipelines/mtdna/#use-mit_single-when","title":"Use <code>mit_single</code> when:","text":"<ul> <li>You need results for one individual.</li> <li>You are adding or reprocessing a single relative.</li> <li>You want faster turnaround for a standalone case.</li> </ul>"},{"location":"pipelines/mtdna/#use-mit_cohort-when","title":"Use <code>mit_cohort</code> when:","text":"<ul> <li>You want a joint variant table across multiple individuals.</li> <li>You are analyzing mtDNA inheritance within a family.</li> <li>You need cohort-level prioritization or downstream analysis.</li> </ul>"},{"location":"pipelines/mtdna/#background-information","title":"Background Information","text":"<p>SG-ADVISER mtDNA builds on MToolbox v1.0 and performs:</p>"},{"location":"pipelines/mtdna/#1-preprocessing-picardtools","title":"1. Preprocessing (PicardTools)","text":"<p>Converts BAM \u2192 FASTQ using: - SortSam.jar - MarkDuplicates.jar - SamFormatConverter.jar (PicardTools)</p>"},{"location":"pipelines/mtdna/#2-alignment","title":"2. Alignment","text":"<ul> <li>Aligns reads to RSRS via <code>mapExome.py</code></li> <li>Uses GSNAP (2015-12-31.v7)</li> </ul>"},{"location":"pipelines/mtdna/#3-variant-calling-annotation-mtoolbox","title":"3. Variant Calling &amp; Annotation (MToolBox)","text":"<p>Pipeline steps include:</p> <ul> <li>mpileup (SAMtools)</li> <li>mtVariantCaller.py</li> <li>VCFoutput.py (with PyVCF)</li> <li>mt-classifier.py (haplogroup prediction)</li> <li>variants_functional_annotation.py</li> <li>prioritization.py</li> <li>summary.py</li> </ul>"},{"location":"pipelines/mtdna/#reference","title":"Reference","text":"<ol> <li>Calabrese C. et al. MToolBox: a highly automated pipeline for heteroplasmy annotation and prioritization analysis of human mitochondrial variants in high-throughput sequencing. Bioinformatics (2014). Read paper</li> </ol>"},{"location":"pipelines/wes_wgs_cohort/","title":"WES/WGS Cohort Joint-Genotyping Pipeline","text":"<p>A user-oriented guide for multi-sample joint genotyping using GenomicsDB, GenotypeGVCFs, and VQSR.</p> See Bash pipeline: <pre><code>#!/usr/bin/env bash\n#\n# w[eg]s_cohort_genomicsdb_with_vqsr.sh\n# Cohort joint-genotyping wrapper using GenomicsDBImport -&gt; GenotypeGVCFs -&gt; VQSR/Hard-filters (GATK 4.6)\n# Last Modified: 2025-10-13\nset -eu\nset -o pipefail\n\nfunction usage {\n  cat &lt;&lt;EOF\nUsage: $0 -m &lt;sample_map.tsv&gt; [-p wes|wgs] [-w &lt;workspace&gt;] [-t &lt;threads&gt;]\n\n  -s  --sample-map   Sample map file for --sample-name-map (required)\n  -p  --pipeline    'wes' (default) or 'wgs'\n  -w  --workspace   GenomicsDB workspace path (default: ./cohort.genomicsdb.&lt;job_id&gt;)\n  -h  --help        Show this help\nEOF\n  exit 1\n}\n\n# Parse args\nPIPELINE=\"wes\"\nWORKSPACE=\"\"\nSAMPLE_MAP=\"\"\n\nwhile [[ $# -gt 0 ]]; do\n  case \"$1\" in\n    -m|--sample-map) SAMPLE_MAP=\"$2\"; shift 2;;\n    -p|--pipeline) PIPELINE=\"$2\"; shift 2;;\n    -w|--workspace) WORKSPACE=\"$2\"; shift 2;;\n    -t) THREADS=\"$2\"; shift 2;;\n    -h|--help) usage ;;\n    *) echo \"Unknown arg: $1\" &gt;&amp;2; usage ;;\n  esac\ndone\n\nif [ -z \"$SAMPLE_MAP\" ]; then\n  echo \"Error: sample_map is required.\" &gt;&amp;2\n  usage\nfi\n\n# Load parameters\nBINDIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\nsource \"$BINDIR/parameters.sh\"\n\n# Prepare output directories and logging\ndir=$(pwd)\nVARCALLDIR=$dir/02_varcall\nLOGDIR=$dir/logs\nmkdir -p \"$VARCALLDIR\"\nmkdir -p \"$LOGDIR\"\n\ncd $VARCALLDIR\n\nLOG=\"$LOGDIR/cohort_joint_genotyping.log\"\n\n# pipeline mode\n# Convert to Uppercase\nPIPELINE=${PIPELINE^^}\nif [[ \"$PIPELINE\" != \"WES\" &amp;&amp; \"$PIPELINE\" != \"WGS\" ]]; then\n  echo \"Error: pipeline must be 'wes' or 'wgs'.\" &gt;&amp;2; exit 1\nfi\n\n# sample count &amp; workspace default naming\nif [ ! -s \"$SAMPLE_MAP\" ]; then\n  echo \"Error: sample_map '$SAMPLE_MAP' not found or empty.\" &gt;&amp;2; exit 1\nfi\nSAMPLE_COUNT=$(wc -l &lt; \"$SAMPLE_MAP\" | tr -d ' ')\nif [ -z \"$WORKSPACE\" ]; then\n  WORKSPACE=\"${WORKSPACE}_${SAMPLE_COUNT}\"\nfi\n\n# interval argument for WES vs WGS\n# Set interval argument for WES vs WGS\nif [ \"$PIPELINE\" = \"WES\" ]; then\n  INTERVAL_ARG=\"-L $INTERVAL_LIST\"\n  echo \"WES mode: restricting to $INTERVAL_LIST\"\nelse\n  INTERVAL_ARG=\"\"\n  echo \"WGS mode: processing whole genome\"\nfi\n\n# Derived output names\nCOHORT_RAW_VCF=\"cohort.gv.raw.vcf.gz\"\nCOHORT_VQSR_SNP=\"cohort.snp.recal.vcf.gz\"\nCOHORT_SNP_TRANCHES=\"cohort.snp.tranches.txt\"\nCOHORT_VQSR_INDEL=\"cohort.indel.recal.vcf.gz\"\nCOHORT_INDEL_TRANCHES=\"cohort.indel.tranches.txt\"\nCOHORT_POST_SNP=\"cohort.post_snp.vcf.gz\"\nCOHORT_POST_VQSR=\"cohort.vqsr.vcf.gz\"\nCOHORT_QC_VCF=\"cohort.gv.QC.vcf.gz\"\n\necho \"## Cohort GenomicsDBImport -&gt; Genotype -&gt; VQSR/Hard-filter\"\necho \"sample_map: $SAMPLE_MAP\"\necho \"pipeline: $PIPELINE\"\necho \"sample_count: $SAMPLE_COUNT\"\necho \"workspace: $WORKSPACE\"\necho \"out_vcf: $COHORT_RAW_VCF\"\necho \"tmpdir: $TMPDIR\"\necho \"log: $LOG\"\necho \"\" | tee -a \"$LOG\"\n\n# -----------------------------------------------------------------------------\n# Step 1: GenomicsDBImport\n# -----------------------------------------------------------------------------\necho \"&gt;&gt;&gt; Step 1: GenomicsDBImport\" | tee -a \"$LOG\"\nmkdir -p \"$(dirname \"$WORKSPACE\")\"\n\nset -x\n\"$GATK4_BIN\" $GATK4_JAVA_OPTS_64G GenomicsDBImport \\\n  --sample-name-map \"$SAMPLE_MAP\" \\\n  --genomicsdb-workspace-path \"$WORKSPACE\" \\\n  --merge-input-intervals true \\\n  $INTERVAL_ARG \\\n  --tmp-dir \"$TMPDIR\" \\\n  2&gt;&gt; \"$LOG\"\nset +x\n\n# -----------------------------------------------------------------------------\n# Step 2: GenotypeGVCFs\n# -----------------------------------------------------------------------------\necho \"&gt;&gt;&gt; Step 2: GenotypeGVCFs\" | tee -a \"$LOG\"\nif [ -z \"${REF:-}\" ]; then\n  echo \"Error: REF not set (expected to be defined in parameters.sh).\" &gt;&amp;2\n  exit 1\nfi\n\nset -x\n\"$GATK4_BIN\" $GATK4_JAVA_OPTS_64G GenotypeGVCFs \\\n  -R \"$REF\" \\\n  -V \"gendb://$WORKSPACE\" \\\n  -O \"$COHORT_RAW_VCF\" \\\n  --stand-call-conf 10 \\\n  --tmp-dir \"$TMPDIR\" \\\n  $INTERVAL_ARG \\\n  2&gt;&gt; \"$LOG\"\nset +x\n\nif [ $? -ne 0 ]; then\n  echo \"ERROR: GenotypeGVCFs failed. See log: $LOG\" &gt;&amp;2\n  exit 1\nfi\necho \"Genotyping completed. Raw cohort VCF: $COHORT_RAW_VCF\" | tee -a \"$LOG\"\n\n# -----------------------------------------------------------------------------\n# Step 3: Count SNPs/INDELs and decide on VQSR\n# -----------------------------------------------------------------------------\necho \"&gt;&gt;&gt; Step 3: Count variants and decide on VQSR\" | tee -a \"$LOG\"\nnSNP=$(zgrep -v '^#' \"$COHORT_RAW_VCF\" | awk 'length($5)==1' | wc -l)\nnINDEL=$(zgrep -v '^#' \"$COHORT_RAW_VCF\" | awk 'length($5)!=1' | wc -l)\nnSNP=$(echo \"$nSNP\" | tr -d ' ')\nnINDEL=$(echo \"$nINDEL\" | tr -d ' ')\necho \"Found SNPs: $nSNP ; INDELs: $nINDEL\" | tee -a \"$LOG\"\n\napply_snp=false\napply_indel=false\nminSNP=${MIN_SNP_FOR_VQSR:-1000}\nminINDEL=${MIN_INDEL_FOR_VQSR:-8000}\n\n# -----------------------------------------------------------------------------\n# Step 4: VariantRecalibrator (SNP)\n# -----------------------------------------------------------------------------\nif (( nSNP &gt;= minSNP )); then\n  echo \"&gt;&gt;&gt; Building SNP VQSR model (VariantRecalibrator)\" | tee -a \"$LOG\"\n  set -x\n  \"$GATK4_BIN\" $GATK4_JAVA_OPTS VariantRecalibrator \\\n    -R \"$REF\" \\\n    -V \"$COHORT_RAW_VCF\" \\\n    $SNP_RES \\\n    -an QD -an MQRankSum -an ReadPosRankSum -an FS -an MQ \\\n    --mode SNP \\\n    -O \"$COHORT_VQSR_SNP\" \\\n    --tranches-file \"$COHORT_SNP_TRANCHES\" \\\n    --max-gaussians 6 \\\n    --tmp-dir \"$TMPDIR\" \\\n    2&gt;&gt; \"$LOG\"\n  set +x\n  apply_snp=true\nelse\n  echo \"Skipping SNP VQSR (found $nSNP &lt; min $minSNP)\" | tee -a \"$LOG\"\nfi\n\n# -----------------------------------------------------------------------------\n# Step 5: VariantRecalibrator (INDEL)\n# -----------------------------------------------------------------------------\nif (( nINDEL &gt;= minINDEL )); then\n  echo \"&gt;&gt;&gt; Building INDEL VQSR model (VariantRecalibrator)\" | tee -a \"$LOG\"\n  set -x\n  \"$GATK4_BIN\" $GATK4_JAVA_OPTS VariantRecalibrator \\\n    -R \"$REF\" \\\n    -V \"$COHORT_RAW_VCF\" \\\n    $INDEL_RES \\\n    -an QD -an FS -an ReadPosRankSum \\\n    --mode INDEL \\\n    -O \"$COHORT_VQSR_INDEL\" \\\n    --tranches-file \"$COHORT_INDEL_TRANCHES\" \\\n    --max-gaussians 4 \\\n    --tmp-dir \"$TMPDIR\" \\\n    2&gt;&gt; \"$LOG\"\n  set +x\n  apply_indel=true\nelse\n  echo \"Skipping INDEL VQSR (found $nINDEL &lt; min $minINDEL)\" | tee -a \"$LOG\"\nfi\n\n# -----------------------------------------------------------------------------\n# Step 6: Apply VQSR (if models exist)\n# -----------------------------------------------------------------------------\necho \"&gt;&gt;&gt; Step 6: Apply VQSR (if available)\" | tee -a \"$LOG\"\ntmp_vcf=\"$COHORT_RAW_VCF\"\nif [ \"$apply_snp\" = true ]; then\n  echo \"Applying SNP recalibration...\" | tee -a \"$LOG\"\n  set -x\n  \"$GATK4_BIN\" $GATK4_JAVA_OPTS ApplyVQSR \\\n    -R \"$REF\" -V \"$tmp_vcf\" \\\n    --recal-file \"$COHORT_VQSR_SNP\" \\\n    --tranches-file \"$COHORT_SNP_TRANCHES\" \\\n    --mode SNP --truth-sensitivity-filter-level 99.0 \\\n    -O \"$COHORT_POST_SNP\" \\\n    --tmp-dir \"$TMPDIR\" \\\n    2&gt;&gt; \"$LOG\"\n  set +x\n  tmp_vcf=\"$COHORT_POST_SNP\"\nfi\n\nif [ \"$apply_indel\" = true ]; then\n  echo \"Applying INDEL recalibration...\" | tee -a \"$LOG\"\n  set -x\n  \"$GATK4_BIN\" $GATK4_JAVA_OPTS ApplyVQSR \\\n    -R \"$REF\" -V \"$tmp_vcf\" \\\n    --recal-file \"$COHORT_VQSR_INDEL\" \\\n    --tranches-file \"$COHORT_INDEL_TRANCHES\" \\\n    --mode INDEL --truth-sensitivity-filter-level 95.0 \\\n    -O \"$COHORT_POST_VQSR\" \\\n    --tmp-dir \"$TMPDIR\" \\\n    2&gt;&gt; \"$LOG\"\n  set +x\n  tmp_vcf=\"$COHORT_POST_VQSR\"\nfi\n\n# -----------------------------------------------------------------------------\n# Step 7: Hard filters &amp; write cohort QC VCF (always run for QC)\n# (filters copied from your single-sample script)\n# -----------------------------------------------------------------------------\necho \"&gt;&gt;&gt; Step 7: Hard-filter &amp; write cohort QC VCF\" | tee -a \"$LOG\"\nset -x\n\"$GATK4_BIN\" $GATK4_JAVA_OPTS VariantFiltration \\\n  -R \"$REF\" \\\n  -V \"$tmp_vcf\" \\\n  --filter-name \"LowQUAL\" --filter-expression \"QUAL &lt; 30.0\" \\\n  --filter-name \"QD2\"        --filter-expression \"QD &lt; 2.0\" \\\n  --filter-name \"FS60\"       --filter-expression \"FS &gt; 60.0\" \\\n  --filter-name \"MQ40\"       --filter-expression \"MQ &lt; 40.0\" \\\n  --filter-name \"MQRS-12.5\"  --filter-expression \"MQRankSum &lt; -12.5\" \\\n  --filter-name \"RPRS-8\"     --filter-expression \"ReadPosRankSum &lt; -8.0\" \\\n  --filter-name \"QD2_indel\"  --filter-expression \"QD &lt; 2.0\" \\\n  --filter-name \"FS200\"      --filter-expression \"FS &gt; 200.0\" \\\n  --filter-name \"RPRS-20\"    --filter-expression \"ReadPosRankSum &lt; -20.0\" \\\n  -O \"$COHORT_QC_VCF\" \\\n  --tmp-dir \"$TMPDIR\" \\\n  2&gt;&gt; \"$LOG\"\nset +x\n\nif [ $? -ne 0 ]; then\n  echo \"ERROR: VariantFiltration (cohort QC) failed. See log: $LOG\" &gt;&amp;2\n  exit 1\nfi\n\necho \"Cohort QC VCF written: $COHORT_QC_VCF\" | tee -a \"$LOG\"\n\n# Final message\necho \"All done. Cohort raw: $COHORT_RAW_VCF ; Cohort QC: $COHORT_QC_VCF\" | tee -a \"$LOG\"\nexit 0\n</code></pre>"},{"location":"pipelines/wes_wgs_cohort/#diagram-cohort-joint-genotyping-workflow","title":"Diagram: Cohort Joint-Genotyping Workflow","text":"flowchart TD     A[\"Input per sample gVCF files\"]     B[\"Import gVCFs into GenomicsDB workspace\"]     C[\"Run GenotypeGVCFs to create cohort raw VCF\"]     D[\"Count SNP and INDEL variants\"]     E[\"Check SNP count threshold\"]     F[\"Check INDEL count threshold\"]     G[\"Build SNP VQSR model\"]     H[\"Build INDEL VQSR model\"]     I[\"Apply SNP VQSR\"]     J[\"Apply INDEL VQSR\"]     K[\"Choose best available VCF\"]     L[\"Apply hard filters\"]     M[\"Write cohort QC VCF\"]      A --&gt; B --&gt; C --&gt; D     D --&gt; E --&gt;|Enough SNPs| G --&gt; I     D --&gt; F --&gt;|Enough INDELs| H --&gt; J     C --&gt; K     I --&gt; K     J --&gt; K     K --&gt; L --&gt; M"},{"location":"pipelines/wes_wgs_cohort/#purpose","title":"Purpose","text":"<p>This pipeline combines many per-sample gVCFs and performs cohort-level joint genotyping, producing a single VCF for all samples with consistent genotype calls and variant filtering.</p> <p>Use this when:</p> <ul> <li>You want consistent genotypes across a family or population.</li> <li>You plan to build VQSR models on cohort-level variant distributions.</li> <li>You are preparing a joint VCF for association or segregation analyses.</li> </ul>"},{"location":"pipelines/wes_wgs_cohort/#inputs","title":"Inputs","text":"<ul> <li>Sample map file (<code>--sample-map</code>):   TSV used by <code>GenomicsDBImport</code> (sample name \u2192 gVCF path).</li> <li>Per-sample gVCFs from the single-sample pipeline.</li> <li>Reference genome (<code>REF</code> from <code>parameters.sh</code>).</li> <li>VQSR resources (SNP and INDEL training sets).</li> <li>Optional interval list for WES mode.</li> </ul>"},{"location":"pipelines/wes_wgs_cohort/#workflow","title":"Workflow","text":""},{"location":"pipelines/wes_wgs_cohort/#1-genomicsdbimport","title":"1. GenomicsDBImport","text":"<ul> <li>Imports all gVCFs into a GenomicsDB workspace.</li> <li>Handles both WES (interval-limited) and WGS (whole genome) modes.</li> <li>Output: on-disk database accessed as <code>gendb://&lt;workspace&gt;</code>.</li> </ul>"},{"location":"pipelines/wes_wgs_cohort/#2-joint-genotyping-genotypegvcfs","title":"2. Joint Genotyping (GenotypeGVCFs)","text":"<ul> <li>Runs <code>GenotypeGVCFs</code> on <code>gendb://&lt;workspace&gt;</code> and the reference.</li> <li> <p>Produces the cohort-level VCF:</p> </li> <li> <p><code>cohort.gv.raw.vcf.gz</code></p> </li> </ul>"},{"location":"pipelines/wes_wgs_cohort/#3-count-variants-and-decide-on-vqsr","title":"3. Count Variants and Decide on VQSR","text":"<ul> <li>Counts SNPs and INDELs in the raw cohort VCF.</li> <li>Compares counts to configurable thresholds:</li> <li><code>MIN_SNP_FOR_VQSR</code> (default 1000)</li> <li><code>MIN_INDEL_FOR_VQSR</code> (default 8000)</li> <li>Determines whether to build SNP and/or INDEL VQSR models.</li> </ul>"},{"location":"pipelines/wes_wgs_cohort/#4-build-vqsr-snp-model","title":"4. Build VQSR SNP Model","text":"<ul> <li>If enough SNPs:</li> <li>Run <code>VariantRecalibrator</code> in SNP mode.</li> <li>Uses training resources and multiple annotations:<ul> <li>QD, FS, MQ, MQRankSum, ReadPosRankSum.</li> </ul> </li> <li>Outputs:<ul> <li><code>cohort.snp.recal.vcf.gz</code></li> <li><code>cohort.snp.tranches.txt</code>.</li> </ul> </li> </ul>"},{"location":"pipelines/wes_wgs_cohort/#5-build-vqsr-indel-model","title":"5. Build VQSR INDEL Model","text":"<ul> <li>If enough INDELs:</li> <li>Run <code>VariantRecalibrator</code> in INDEL mode.</li> <li>Uses annotations:<ul> <li>QD, FS, ReadPosRankSum.</li> </ul> </li> <li>Outputs:<ul> <li><code>cohort.indel.recal.vcf.gz</code></li> <li><code>cohort.indel.tranches.txt</code>.</li> </ul> </li> </ul>"},{"location":"pipelines/wes_wgs_cohort/#6-apply-vqsr","title":"6. Apply VQSR","text":"<ul> <li>If SNP model exists:</li> <li>Apply SNP VQSR \u2192 <code>cohort.post_snp.vcf.gz</code>.</li> <li>If INDEL model exists:</li> <li>Apply INDEL VQSR \u2192 <code>cohort.vqsr.vcf.gz</code>.</li> </ul> <p>The best available VCF (VQSR-filtered or raw) is used as input to the next step.</p>"},{"location":"pipelines/wes_wgs_cohort/#7-hard-filtering-and-qc-vcf","title":"7. Hard Filtering and QC VCF","text":"<ul> <li>Run <code>VariantFiltration</code> with a set of hard filters on:</li> <li>QUAL, QD, FS, MQ, MQRankSum, ReadPosRankSum.</li> <li>Output: <code>cohort.gv.QC.vcf.gz</code>.</li> </ul> <p>This QC VCF is the recommended cohort VCF for downstream analysis.</p>"},{"location":"pipelines/wes_wgs_cohort/#output-files","title":"Output Files","text":"File Description <code>cohort.gv.raw.vcf.gz</code> Raw cohort joint-genotyped VCF <code>cohort.snp.recal.vcf.gz</code> SNP VQSR model VCF <code>cohort.snp.tranches.txt</code> SNP VQSR tranches and diagnostics <code>cohort.indel.recal.vcf.gz</code> INDEL VQSR model VCF <code>cohort.indel.tranches.txt</code> INDEL VQSR tranches and diagnostics <code>cohort.post_snp.vcf.gz</code> VCF after applying SNP VQSR <code>cohort.vqsr.vcf.gz</code> VCF after applying SNP and INDEL VQSR <code>cohort.gv.QC.vcf.gz</code> Final hard-filtered cohort QC VCF (recommended) <code>logs/cohort_joint_genotyping.log</code> Main log file for the cohort pipeline"},{"location":"pipelines/wes_wgs_cohort/#when-to-use-this-pipeline","title":"When to Use This Pipeline","text":"<ul> <li>After you have gVCFs from the single-sample pipeline.</li> <li>When you need a single VCF for all samples for:</li> <li>Family-based segregation analysis.</li> <li>Case/control or population cohorts.</li> <li>Downstream tools that expect joint genotypes.</li> </ul>"},{"location":"pipelines/wes_wgs_single/","title":"WES/WGS Single-Sample Pipeline","text":"<p>A user-focused guide to processing whole-exome (WES) and whole-genome (WGS) data using GATK Best Practices.</p> BashSnakemake See Bash pipeline: <pre><code>#!/usr/bin/env bash\n#\n#   WGS/WES pipeline Bash script (GATK4.6 best-practices edition).\n#   Last Modified: 2025-05-02\n#\n# README:\n#   This script runs either WES or WGS pipelines based on the '-p' mode flag.\n#   Mode 'wes': uses provided exome interval list to restrict variant calling.\n#   Mode 'wgs': processes the entire genome (no intervals).\n#   Common steps (GATK Best Practices):\n#     1) Align &amp; add read groups\n#     2) Merge lane-level BAMs\n#     3) Mark duplicates\n#     4) Base Quality Score Recalibration (BQSR)\n#     5) HaplotypeCaller (per-sample gVCF)\n#     6) GenotypeGVCFs (raw VCF)\n#     7) VariantRecalibrator (VQSR) if variant counts suffice\n#     8) Apply VQSR models or fallback to hard filters\n#     9) Hard-filter &amp; write QC VCF\n#    10) Coverage stats &amp; sex determination\n\nset -eu\n\nCLEANUP_BAM=false\n\nfunction usage {\n    cat &lt;&lt;EOF\nUsage: $0 -t &lt;n_threads&gt; -p &lt;wes|wgs&gt; [-c]\n\n  -t, --threads         Number of CPU threads for GATK tools\n  -p, --pipeline        Pipeline mode: 'wes' or 'wgs'\n  -c, --cleanup-bam     If set, delete all 01_bam/*.bam when done (default: off)\nEOF\n    exit 1\n}\n\n# Parse command-line arguments\nwhile [[ $# -gt 0 ]]; do\n  case \"$1\" in\n    -t|--threads)\n      THREADS=\"$2\"\n      shift 2\n      ;;\n    -p|--pipeline)\n      PIPELINE=\"$2\"\n      shift 2\n      ;;\n    -c|--cleanup-bam)\n      CLEANUP_BAM=true\n      shift\n      ;;\n    *)\n      usage\n      ;;\n  esac\ndone\n\n# make sure the required ones really got set\nif [ -z \"${THREADS:-}\" ] || [ -z \"${PIPELINE:-}\" ]; then\n  usage\nfi\n\n# Convert to Uppercase\nPIPELINE=${PIPELINE^^}\nif [[ \"$PIPELINE\" != \"WES\" &amp;&amp; \"$PIPELINE\" != \"WGS\" ]]; then\n  echo \"Error: Mode must be 'wes' or 'wgs'.\" &gt;&amp;2\n  usage\nfi\n\n# Load GATK parameters (reference, known-sites, interval list, tool paths)\nBINDIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\nsource \"$BINDIR/parameters.sh\"\n\n# Prepare output directories\ndir=$(pwd)\nBAMDIR=$dir/01_bam\nVARCALLDIR=$dir/02_varcall\nSTATSDIR=$dir/03_stats\nLOGDIR=$dir/logs\nmkdir -p \"$BAMDIR\" \"$VARCALLDIR\" \"$STATSDIR\" \"$LOGDIR\"\n\n# Determine sample ID &amp; log file\nrawid=$(basename \"$(dirname \"$PWD\")\")\nid=${rawid%%_*}\nLOG=$LOGDIR/${id}.log\n\n# Set interval argument for WES vs WGS\nif [ \"$PIPELINE\" = \"WES\" ]; then\n  INTERVAL_ARG=\"-L $INTERVAL_LIST\"\n  echo \"WES mode: restricting to $INTERVAL_LIST\"\nelse\n  INTERVAL_ARG=\"\"\n  echo \"WGS mode: processing whole genome\"\nfi\n\n#------------------------------------------------------------------------------\n# STEP 1: Align &amp; Add Read Groups (per-lane BAMs)\n# Theory: Read groups are required for GATK to tag sample, library, and platform\n# information, enabling per-sample metrics and proper handling of multi-library data.\n#------------------------------------------------------------------------------\necho \"&gt;&gt;&gt; STEP 1: Align &amp; add read groups\"\nfor R1 in ../*_R1_*fastq.gz; do\n  fn=$(basename \"$R1\" .fastq.gz)\n  base=${fn%_R1*}\n  R2=${R1/_R1_/_R2_}\n\n  SAMPLE=$(echo \"$base\" | cut -d'_' -f1-2)\n  LANE=$(echo   \"$base\" | cut -d'_' -f3)\n  # ensure RGIDs are unique by appending a timestamp\n  RGID=\"${SAMPLE}.${LANE}.$(date +%s)\"\n  RGPU=\"${SAMPLE}.${LANE}.unit1\"\n\n  out_bam=\"$BAMDIR/${base}.rg.bam\"\n  echo \"Aligning $fn -&gt; $(basename \"$out_bam\")\"\n\n  # Align and drop secondary (0x100) &amp; supplementary (0x800) alignments before RG tagging\n  #$BWA mem -M -t \"$THREADS\" \"$REFGZ\" \"$R1\" \"$R2\" \\\n  #  | $SAM view -b -F 0x900 - \\\n  # ...\n\n  # Align without filtering (keep all alignments):\n  $BWA mem -M -t \"$THREADS\" \"$REFGZ\" \"$R1\" \"$R2\" \\\n    | \"$GATK4_BIN\" $GATK4_JAVA_OPTS AddOrReplaceReadGroups \\\n        --INPUT /dev/stdin \\\n        --OUTPUT \"$out_bam\" \\\n        --TMP_DIR \"$TMPDIR\" \\\n        --RGPL ILLUMINA \\\n        --RGLB sureselect \\\n        --RGSM \"$SAMPLE\" \\\n        --RGID \"$RGID\" \\\n        --RGPU \"$RGPU\" \\\n    2&gt;&gt; \"$LOG\"\ndone\n\n#------------------------------------------------------------------------------\n# STEP 2: Merge Lane-level BAMs\n# Theory: Merging ensures downstream duplicate marking and BQSR\n# operate on the full sample, minimizing lane-specific biases.\n#------------------------------------------------------------------------------\necho \"&gt;&gt;&gt; STEP 2: Merge lane-level BAMs\"\nrg_bams=( $BAMDIR/*.rg.bam )\nset -x\n\"$GATK4_BIN\" $GATK4_JAVA_OPTS MergeSamFiles \\\n  ${rg_bams[@]/#/-I } \\\n  -O \"$BAMDIR/${id}.rg.merged.bam\" \\\n  --CREATE_INDEX true --VALIDATION_STRINGENCY SILENT --TMP_DIR \"$TMPDIR\" \\\n  2&gt;&gt; \"$LOG\"\nset +x\n\n#------------------------------------------------------------------------------\n# STEP 3: Mark Duplicates\n# Theory: PCR/optical duplicates inflate support for artefactual variants,\n# so marking them reduces false positives in variant calling.\n#------------------------------------------------------------------------------\necho \"&gt;&gt;&gt; STEP 3: Mark duplicates\"\nset -x\n\"$GATK4_BIN\" $GATK4_JAVA_OPTS MarkDuplicates \\\n  -I \"$BAMDIR/${id}.rg.merged.bam\" \\\n  -O \"$BAMDIR/${id}.rg.merged.dedup.bam\" \\\n  --METRICS_FILE \"$BAMDIR/${id}.rg.merged.dedup.metrics.txt\" \\\n  --CREATE_INDEX true --TMP_DIR \"$TMPDIR\" \\\n  2&gt;&gt; \"$LOG\"\nset +x\n$SAM index \"$BAMDIR/${id}.rg.merged.dedup.bam\"\n\n#------------------------------------------------------------------------------\n# STEP 4: Base Quality Score Recalibration (BQSR)\n# Theory: Models systematic error in reported base qualities using known-sites,\n# improving the accuracy of variant calls by correcting quality scores.\n#------------------------------------------------------------------------------\necho \"&gt;&gt;&gt; STEP 4: Base recalibration\"\nbqsr_table=\"$BAMDIR/${id}.rg.merged.dedup.recal.table\"\nset -x\n\"$GATK4_BIN\" $GATK4_JAVA_OPTS BaseRecalibrator \\\n  -R \"$REF\" \\\n  -I \"$BAMDIR/${id}.rg.merged.dedup.bam\" \\\n  --known-sites \"$dbSNP\" --known-sites \"$MILLS_INDELS\" --known-sites \"$KG_INDELS\" \\\n  -O \"$bqsr_table\" --tmp-dir \"$TMPDIR\" 2&gt;&gt; \"$LOG\"\n\n\"$GATK4_BIN\" $GATK4_JAVA_OPTS ApplyBQSR \\\n  -R \"$REF\" -I \"$BAMDIR/${id}.rg.merged.dedup.bam\" \\\n  --bqsr-recal-file \"$bqsr_table\" \\\n  -O \"$BAMDIR/${id}.rg.merged.dedup.recal.bam\" --tmp-dir \"$TMPDIR\" 2&gt;&gt; \"$LOG\"\n$SAM index \"$BAMDIR/${id}.rg.merged.dedup.recal.bam\"\nset +x\n\n#------------------------------------------------------------------------------\n# STEP 5: HaplotypeCaller -&gt; gVCF\n# Theory: Local de Bruijn graph assembly per active region improves indel calling;\n# emits reference-confidence gVCF required for joint genotyping.\n#------------------------------------------------------------------------------\necho \"&gt;&gt;&gt; STEP 5: HaplotypeCaller -&gt; gVCF\"\nset -x\n\"$GATK4_BIN\" $GATK4_JAVA_OPTS HaplotypeCaller \\\n  -R \"$REF\" -I \"$BAMDIR/${id}.rg.merged.dedup.recal.bam\" \\\n  -O \"$VARCALLDIR/${id}.hc.g.vcf.gz\" \\\n  $INTERVAL_ARG \\\n  --native-pair-hmm-threads \"$THREADS\" -ERC GVCF 2&gt;&gt; \"$LOG\"\nset +x\n\n#------------------------------------------------------------------------------\n# STEP 6: GenotypeGVCFs -&gt; Raw VCF\n# Theory: Jointly genotype one or more gVCFs to produce high-confidence sample VCF.\n#------------------------------------------------------------------------------\necho \"&gt;&gt;&gt; STEP 6: GenotypeGVCFs -&gt; raw VCF\"\nset -x\n\"$GATK4_BIN\" $GATK4_JAVA_OPTS GenotypeGVCFs \\\n  -R \"$REF\" -V \"$VARCALLDIR/${id}.hc.g.vcf.gz\" \\\n  -O \"$VARCALLDIR/${id}.hc.raw.vcf.gz\" --stand-call-conf 10 2&gt;&gt; \"$LOG\"\nset +x\n\n#------------------------------------------------------------------------------\n# STEP 7: Build VQSR Models\n# Theory: Use Gaussian mixture models on variant annotations\n# to distinguish true variants from artefacts; requires sufficient counts.\n#------------------------------------------------------------------------------\necho \"&gt;&gt;&gt; STEP 7: Optional VQSR if enough variants\"\nnSNP=$(zgrep -v '^#' \"$VARCALLDIR/${id}.hc.raw.vcf.gz\" | awk 'length($5)==1' | wc -l)\nnINDEL=$(zgrep -v '^#' \"$VARCALLDIR/${id}.hc.raw.vcf.gz\" | awk 'length($5)!=1' | wc -l)\nminSNP=1000; minINDEL=8000; apply_snp=false; apply_indel=false\n\nif (( nSNP &gt;= minSNP )); then\n  \"$GATK4_BIN\" $GATK4_JAVA_OPTS VariantRecalibrator \\\n    -R \"$REF\" \\\n    -V \"$VARCALLDIR/${id}.hc.raw.vcf.gz\" \\\n    $SNP_RES \\\n    -an QD -an MQRankSum -an ReadPosRankSum -an FS -an MQ \\\n    --mode SNP \\\n    -O \"$VARCALLDIR/${id}.hc.snp.recal.vcf.gz\" \\\n    --tranches-file \"$VARCALLDIR/${id}.hc.snp.tranches.txt\" \\\n    --max-gaussians 6 2&gt;&gt; \"$LOG\"\n  apply_snp=true\nfi\n\nif (( nINDEL &gt;= minINDEL )); then\n  \"$GATK4_BIN\" $GATK4_JAVA_OPTS VariantRecalibrator \\\n    -R \"$REF\" \\\n    -V \"$VARCALLDIR/${id}.hc.raw.vcf.gz\" \\\n    $INDEL_RES \\\n    -an QD -an FS -an ReadPosRankSum \\\n    --mode INDEL \\\n    -O \"$VARCALLDIR/${id}.hc.indel.recal.vcf.gz\" \\\n    --tranches-file \"$VARCALLDIR/${id}.hc.indel.tranches.txt\" \\\n    --max-gaussians 4 2&gt;&gt; \"$LOG\"\n  apply_indel=true\nfi\n\n#------------------------------------------------------------------------------\n# STEP 8: Apply VQSR Models or Fallback\n#------------------------------------------------------------------------------\necho \"&gt;&gt;&gt; STEP 8: Apply VQSR or fallback\"\ntmp_vcf=\"$VARCALLDIR/${id}.hc.raw.vcf.gz\"\n\nif [ \"$apply_snp\" = true ]; then\n  \"$GATK4_BIN\" $GATK4_JAVA_OPTS ApplyVQSR \\\n    -R \"$REF\" -V \"$tmp_vcf\" \\\n    --recal-file \"$VARCALLDIR/${id}.hc.snp.recal.vcf.gz\" \\\n    --tranches-file \"$VARCALLDIR/${id}.hc.snp.tranches.txt\" \\\n    --mode SNP --truth-sensitivity-filter-level 99.0 \\\n    -O \"$VARCALLDIR/${id}.hc.post_snp.vcf.gz\" 2&gt;&gt; \"$LOG\"\n  tmp_vcf=\"$VARCALLDIR/${id}.hc.post_snp.vcf.gz\"\nfi\n\nif [ \"$apply_indel\" = true ]; then\n  \"$GATK4_BIN\" $GATK4_JAVA_OPTS ApplyVQSR \\\n    -R \"$REF\" -V \"$tmp_vcf\" \\\n    --recal-file \"$VARCALLDIR/${id}.hc.indel.recal.vcf.gz\" \\\n    --tranches-file \"$VARCALLDIR/${id}.hc.indel.tranches.txt\" \\\n    --mode INDEL --truth-sensitivity-filter-level 95.0 \\\n    -O \"$VARCALLDIR/${id}.hc.vqsr.vcf.gz\" 2&gt;&gt; \"$LOG\"\n  tmp_vcf=\"$VARCALLDIR/${id}.hc.vqsr.vcf.gz\"\nfi\n\n#------------------------------------------------------------------------------\n# STEP 9: Hard-Filter &amp; Write QC VCF\n# Theory: Apply recommended hard filters on annotations when VQSR isn't applied or as QC.\n#   SNPs: QD&lt;2.0, FS&gt;60.0, MQ&lt;40.0, MQRankSum&lt;-12.5, ReadPosRankSum&lt;-8.0\n#   INDELs: QD&lt;2.0, FS&gt;200.0, ReadPosRankSum&lt;-20.0\n#------------------------------------------------------------------------------\necho \"&gt;&gt;&gt; STEP 9: Hard-filter &amp; write QC.vcf\"\n\"$GATK4_BIN\" $GATK4_JAVA_OPTS VariantFiltration \\\n  -R \"$REF\" \\\n  -V \"$tmp_vcf\" \\\n  --filter-name \"LowQUAL\" --filter-expression \"QUAL &lt; 30.0\" \\\n  --filter-name \"QD2\"        --filter-expression \"QD &lt; 2.0\" \\\n  --filter-name \"FS60\"       --filter-expression \"FS &gt; 60.0\" \\\n  --filter-name \"MQ40\"       --filter-expression \"MQ &lt; 40.0\" \\\n  --filter-name \"MQRS-12.5\"  --filter-expression \"MQRankSum &lt; -12.5\" \\\n  --filter-name \"RPRS-8\"     --filter-expression \"ReadPosRankSum &lt; -8.0\" \\\n  --filter-name \"QD2_indel\"  --filter-expression \"QD &lt; 2.0\" \\\n  --filter-name \"FS200\"      --filter-expression \"FS &gt; 200.0\" \\\n  --filter-name \"RPRS-20\"    --filter-expression \"ReadPosRankSum &lt; -20.0\" \\\n  -O \"$VARCALLDIR/${id}.hc.QC.vcf.gz\" \\\n  2&gt;&gt; \"$LOG\"\n\n#------------------------------------------------------------------------------\n# STEP 10: Coverage Stats &amp; Sex Determination\n#------------------------------------------------------------------------------\necho \"&gt;&gt;&gt; STEP 10: Coverage &amp; Sex Determination\" 2&gt;&gt; \"$LOG\"\n\n# choose chromosome 1 naming\nif [[ \"$REF\" == *b37*.fasta ]]; then\n  chrN=1\nelse\n  chrN=chr1\nfi\n\nbam_raw=\"$BAMDIR/${id}.rg.merged.dedup.bam\"\nbam_recal=\"$BAMDIR/${id}.rg.merged.dedup.recal.bam\"\nout_raw=\"$STATSDIR/${chrN}.raw.bam\"\nout_dedup=\"$STATSDIR/${chrN}.dedup.bam\"\n\n# extract chr1 (or \u201c1\u201d) reads and redirect errors\n$SAM view -b \"$bam_raw\" \"$chrN\"  &gt; \"$out_raw\"   2&gt;&gt; \"$LOG\"\n$SAM view -b \"$bam_recal\" \"$chrN\"  &gt; \"$out_dedup\" 2&gt;&gt; \"$LOG\"\n\n# index them, also logging STDERR\n$SAM index \"$out_raw\"    2&gt;&gt; \"$LOG\"\n$SAM index \"$out_dedup\"  2&gt;&gt; \"$LOG\"\n\n# run coverage &amp; sex scripts: STDOUT -&gt; stats file, STDERR -&gt; main log\n\"$BINDIR\"/coverage.sh \"$id\" \"$out_raw\" \"$out_dedup\" \"$PIPELINE\" \\\n    &gt; \"$STATSDIR/${id}.coverage.txt\" 2&gt;&gt; \"$LOG\"\n\"$BINDIR\"/vcf2sex.sh \"$VARCALLDIR/${id}.hc.QC.vcf.gz\" \\\n    &gt; \"$STATSDIR/${id}.sex.txt\" 2&gt;&gt; \"$LOG\"\n\n# Delete $STATSDIR/*bam\nrm \"$out_raw\" \"$out_dedup\" \"$out_raw.bai\" \"$out_dedup.bai\"\n\n#------------------------------------------------------------------------------\n# STEP 11: Cleanup BAMs (optional)\n#------------------------------------------------------------------------------\nif [ \"$CLEANUP_BAM\" = true ]; then\n  echo \"&gt;&gt;&gt; STEP 11: Cleanup BAMs\" 2&gt;&gt; \"$LOG\"\n  # -f silences \u201cno such file\u201d errors if the glob is empty\n  rm -f \"$BAMDIR\"/*.{bam,bai} 2&gt;&gt; \"$LOG\"\nfi\n\n# End\necho \"All done! QC VCF: $VARCALLDIR/${id}.hc.QC.vcf.gz\"\n</code></pre> See Snakemake pipeline: <pre><code># wes_single.smk - bash-replica of wes_single.sh (GATK4.6) with per-rule logs\nimport os\nimport glob\nimport platform\nimport shlex\nfrom pathlib import Path\n\nsnakefile_dir = Path(workflow.snakefile).parent\nconfigfile: str(snakefile_dir / \"config.yaml\")\n\n# -----------------------------\n# Environment (match bash)\n# -----------------------------\nos.environ[\"TMPDIR\"] = config[\"tmpdir\"]\nos.environ[\"LC_ALL\"] = \"C\"\nos.environ[\"GATK_DISABLE_AUTO_S3_UPLOAD\"] = \"true\"\n\n# -----------------------------\n# Config / tools\n# -----------------------------\nDATADIR  = config[\"datadir\"]\nDBDIR    = config[\"dbdir\"].format(datadir=DATADIR)\nNGSUTILS = config[\"ngsutils\"].format(datadir=DATADIR)\nTMPDIR   = config[\"tmpdir\"].format(datadir=DATADIR)\nMEM      = config.get(\"mem\", \"8G\")\n\nPIPELINE = config.get(\"pipeline\", \"wes\").lower()\nif PIPELINE not in (\"wes\", \"wgs\"):\n    raise ValueError(\"config[pipeline] must be 'wes' or 'wgs'\")\n\nCLEANUP_BAM = bool(config.get(\"cleanup_bam\", False))\n\nTHREADS = workflow.cores or int(config.get(\"threads\", 4))\n\nARCH = platform.machine()\nif ARCH == \"aarch64\":\n    JAVA = config[\"java\"][\"aarch64\"]\n    BWA  = config[\"tools\"][\"aarch64\"][\"bwa\"].format(ngsutils=NGSUTILS)\n    SAM  = config[\"tools\"][\"aarch64\"][\"samtools\"].format(ngsutils=NGSUTILS)\nelse:\n    JAVA = config[\"java\"][\"amd64\"]\n    BWA  = config[\"tools\"][\"amd64\"][\"bwa\"].format(ngsutils=NGSUTILS)\n    SAM  = config[\"tools\"][\"amd64\"][\"samtools\"].format(ngsutils=NGSUTILS)\n\nGATK4 = config[\"gatk4_cmd\"].format(ngsutils=NGSUTILS, mem=MEM)\n\nCOV     = str(snakefile_dir / \"coverage.sh\")\nVCF2SEX = str(snakefile_dir / \"vcf2sex.sh\")\n\nbundle       = config[\"bundle\"].format(dbdir=DBDIR)\nREF          = config[\"ref\"].format(bundle=bundle)\nREFGZ        = config[\"refgz\"].format(bundle=bundle)\ndbSNP        = config[\"dbsnp\"].format(dbdir=DBDIR)\nMILLS_INDELS = config[\"mills_indels\"].format(bundle=bundle)\nKG_INDELS    = config[\"kg_indels\"].format(bundle=bundle)\nHAPMAP       = config[\"hapmap\"].format(bundle=bundle)\nOMNI         = config[\"omni\"].format(bundle=bundle)\n\nSNP_RES   = config[\"snp_res\"].format(hapmap=HAPMAP, omni=OMNI, dbsnp=dbSNP)\nINDEL_RES = config[\"indel_res\"].format(mills_indels=MILLS_INDELS)\n\nINTERVAL_LIST = config[\"interval_list\"].format(bundle=bundle)\nINTERVAL_ARG  = f\"-L {shlex.quote(INTERVAL_LIST)}\" if PIPELINE == \"wes\" else \"\"\n\nMIN_SNP_FOR_VQSR   = int(config.get(\"min_snp_for_vqsr\", 1000))\nMIN_INDEL_FOR_VQSR = int(config.get(\"min_indel_for_vqsr\", 8000))\n\n# Output dirs\nBAMDIR     = \"01_bam\"\nVARCALLDIR = \"02_varcall\"\nSTATSDIR   = \"03_stats\"\nLOGDIR     = \"logs\"\nfor d in (BAMDIR, VARCALLDIR, STATSDIR, LOGDIR):\n    os.makedirs(d, exist_ok=True)\n\n# -----------------------------\n# Sample ID (match bash)\n# -----------------------------\nrawid = Path.cwd().parent.name\nID = rawid.split(\"_\", 1)[0]\n\n# -----------------------------\n# FASTQ pairs (match bash glob)\n# -----------------------------\nFASTQ_DIR = \"../\"\nFASTQ_R1 = sorted(glob.glob(os.path.join(FASTQ_DIR, \"*_R1_*fastq.gz\")))\nif not FASTQ_R1:\n    raise ValueError(\"No FASTQs found matching ../*_R1_*fastq.gz\")\n\nFASTQ_BASES = []\nFASTQ_DICT = {}\nfor r1 in FASTQ_R1:\n    r2 = r1.replace(\"_R1_\", \"_R2_\")\n    base = os.path.basename(r1).replace(\".fastq.gz\", \"\")\n    base = base.split(\"_R1_\", 1)[0]\n    FASTQ_BASES.append(base)\n    FASTQ_DICT[base] = {\"r1\": r1, \"r2\": r2}\n\nRG_BAMS = [os.path.join(BAMDIR, f\"{b}.rg.bam\") for b in FASTQ_BASES]\n\nCHR1 = \"1\" if \"b37\" in str(REF) else \"chr1\"\n\n# -----------------------------\n# Targets\n# -----------------------------\nFINAL_INPUTS = [\n    os.path.join(VARCALLDIR, f\"{ID}.hc.QC.vcf.gz\"),\n    os.path.join(STATSDIR,   f\"{ID}.coverage.txt\"),\n    os.path.join(STATSDIR,   f\"{ID}.sex.txt\"),\n]\nif CLEANUP_BAM:\n    FINAL_INPUTS.append(os.path.join(LOGDIR, f\"{ID}.cleanup.done\"))\n\nrule all:\n    input:\n        FINAL_INPUTS\n\n# -----------------------------\n# STEP 1: Align &amp; AddReadGroups\n# -----------------------------\nrule align_rg:\n    input:\n        r1=lambda wc: FASTQ_DICT[wc.base][\"r1\"],\n        r2=lambda wc: FASTQ_DICT[wc.base][\"r2\"],\n    output:\n        bam=os.path.join(BAMDIR, \"{base}.rg.bam\"),\n    threads: THREADS\n    log:\n        os.path.join(LOGDIR, f\"{ID}.01_align_rg.{{base}}.log\")\n    shell:\n        r\"\"\"\n        set -eu\n        SAMPLE=$(echo {wildcards.base} | cut -d'_' -f1-2)\n        LANE=$(echo   {wildcards.base} | cut -d'_' -f3)\n        RGID=\"${{SAMPLE}}.${{LANE}}.$(date +%s)\"\n        RGPU=\"${{SAMPLE}}.${{LANE}}.unit1\"\n\n        {BWA} mem -M -t {threads} {REFGZ} {input.r1} {input.r2} \\\n          | {GATK4} AddOrReplaceReadGroups \\\n              --INPUT /dev/stdin \\\n              --OUTPUT {output.bam} \\\n              --TMP_DIR {TMPDIR} \\\n              --RGPL ILLUMINA \\\n              --RGLB sureselect \\\n              --RGSM \"$SAMPLE\" \\\n              --RGID \"$RGID\" \\\n              --RGPU \"$RGPU\" \\\n          2&gt;&gt; {log}\n        \"\"\"\n\n# -----------------------------\n# STEP 2: Merge lane BAMs\n# -----------------------------\nrule merge_bams:\n    input:\n        rg_bams=RG_BAMS\n    output:\n        merged=os.path.join(BAMDIR, f\"{ID}.rg.merged.bam\")\n    params:\n        merge_inputs=lambda wc, input: \" \".join([f\"-I {b}\" for b in input.rg_bams])\n    log:\n        os.path.join(LOGDIR, f\"{ID}.02_merge_bams.log\")\n    shell:\n        r\"\"\"\n        set -eu\n        {GATK4} MergeSamFiles \\\n          {params.merge_inputs} \\\n          -O {output.merged} \\\n          --CREATE_INDEX true \\\n          --VALIDATION_STRINGENCY SILENT \\\n          --TMP_DIR {TMPDIR} \\\n          2&gt;&gt; {log}\n        \"\"\"\n\n# -----------------------------\n# STEP 3: MarkDuplicates (+ samtools index)\n# -----------------------------\nrule mark_duplicates:\n    input:\n        merged=os.path.join(BAMDIR, f\"{ID}.rg.merged.bam\")\n    output:\n        dedup=os.path.join(BAMDIR, f\"{ID}.rg.merged.dedup.bam\"),\n        metrics=os.path.join(BAMDIR, f\"{ID}.rg.merged.dedup.metrics.txt\")\n    log:\n        os.path.join(LOGDIR, f\"{ID}.03_mark_duplicates.log\")\n    shell:\n        r\"\"\"\n        set -eu\n        {GATK4} MarkDuplicates \\\n          -I {input.merged} \\\n          -O {output.dedup} \\\n          --METRICS_FILE {output.metrics} \\\n          --CREATE_INDEX true \\\n          --TMP_DIR {TMPDIR} \\\n          2&gt;&gt; {log}\n        {SAM} index {output.dedup} 2&gt;&gt; {log}\n        \"\"\"\n\n# -----------------------------\n# STEP 4: BQSR\n# -----------------------------\nrule bqsr:\n    input:\n        dedup=os.path.join(BAMDIR, f\"{ID}.rg.merged.dedup.bam\")\n    output:\n        table=os.path.join(BAMDIR, f\"{ID}.rg.merged.dedup.recal.table\"),\n        recal=os.path.join(BAMDIR, f\"{ID}.rg.merged.dedup.recal.bam\")\n    log:\n        os.path.join(LOGDIR, f\"{ID}.04_bqsr.log\")\n    shell:\n        r\"\"\"\n        set -eu\n        {GATK4} BaseRecalibrator \\\n          -R {REF} \\\n          -I {input.dedup} \\\n          --known-sites {dbSNP} \\\n          --known-sites {MILLS_INDELS} \\\n          --known-sites {KG_INDELS} \\\n          -O {output.table} \\\n          --tmp-dir {TMPDIR} \\\n          2&gt;&gt; {log}\n\n        {GATK4} ApplyBQSR \\\n          -R {REF} \\\n          -I {input.dedup} \\\n          --bqsr-recal-file {output.table} \\\n          -O {output.recal} \\\n          --tmp-dir {TMPDIR} \\\n          2&gt;&gt; {log}\n\n        {SAM} index {output.recal} 2&gt;&gt; {log}\n        \"\"\"\n\n# -----------------------------\n# STEP 5: HaplotypeCaller -&gt; gVCF\n# -----------------------------\nrule haplotypecaller:\n    input:\n        recal=os.path.join(BAMDIR, f\"{ID}.rg.merged.dedup.recal.bam\")\n    output:\n        gvcf=os.path.join(VARCALLDIR, f\"{ID}.hc.g.vcf.gz\")\n    threads: THREADS\n    log:\n        os.path.join(LOGDIR, f\"{ID}.05_haplotypecaller.log\")\n    shell:\n        r\"\"\"\n        set -eu\n        {GATK4} HaplotypeCaller \\\n          -R {REF} \\\n          -I {input.recal} \\\n          -O {output.gvcf} \\\n          {INTERVAL_ARG} \\\n          --native-pair-hmm-threads {threads} \\\n          -ERC GVCF \\\n          2&gt;&gt; {log}\n        \"\"\"\n\n# -----------------------------\n# STEP 6: GenotypeGVCFs -&gt; raw VCF\n# -----------------------------\nrule genotype_gvcfs:\n    input:\n        gvcf=os.path.join(VARCALLDIR, f\"{ID}.hc.g.vcf.gz\")\n    output:\n        raw=os.path.join(VARCALLDIR, f\"{ID}.hc.raw.vcf.gz\")\n    log:\n        os.path.join(LOGDIR, f\"{ID}.06_genotype_gvcfs.log\")\n    shell:\n        r\"\"\"\n        set -eu\n        {GATK4} GenotypeGVCFs \\\n          -R {REF} \\\n          -V {input.gvcf} \\\n          -O {output.raw} \\\n          --stand-call-conf 10 \\\n          2&gt;&gt; {log}\n        \"\"\"\n\n# -----------------------------\n# STEPS 7-9: Conditional VQSR + always QC VariantFiltration\n# -----------------------------\nrule vqsr_and_qc:\n    input:\n        raw=os.path.join(VARCALLDIR, f\"{ID}.hc.raw.vcf.gz\")\n    output:\n        qc=os.path.join(VARCALLDIR, f\"{ID}.hc.QC.vcf.gz\")\n    log:\n        os.path.join(LOGDIR, f\"{ID}.07_vqsr_and_qc.log\")\n    shell:\n        r\"\"\"\n        set -eu\n\n        rawvcf={input.raw}\n        tmp_vcf=\"$rawvcf\"\n\n        nSNP=$(zgrep -v '^#' \"$rawvcf\" | awk 'length($5)==1' | wc -l | tr -d ' ')\n        nINDEL=$(zgrep -v '^#' \"$rawvcf\" | awk 'length($5)!=1' | wc -l | tr -d ' ')\n\n        apply_snp=false\n        apply_indel=false\n\n        if [ \"$nSNP\" -ge \"{MIN_SNP_FOR_VQSR}\" ]; then\n          {GATK4} VariantRecalibrator \\\n            -R {REF} \\\n            -V \"$rawvcf\" \\\n            {SNP_RES} \\\n            -an QD -an MQRankSum -an ReadPosRankSum -an FS -an MQ \\\n            --mode SNP \\\n            -O {VARCALLDIR}/{ID}.hc.snp.recal.vcf.gz \\\n            --tranches-file {VARCALLDIR}/{ID}.hc.snp.tranches.txt \\\n            --max-gaussians 6 \\\n            2&gt;&gt; {log}\n          apply_snp=true\n        fi\n\n        if [ \"$nINDEL\" -ge \"{MIN_INDEL_FOR_VQSR}\" ]; then\n          {GATK4} VariantRecalibrator \\\n            -R {REF} \\\n            -V \"$rawvcf\" \\\n            {INDEL_RES} \\\n            -an QD -an FS -an ReadPosRankSum \\\n            --mode INDEL \\\n            -O {VARCALLDIR}/{ID}.hc.indel.recal.vcf.gz \\\n            --tranches-file {VARCALLDIR}/{ID}.hc.indel.tranches.txt \\\n            --max-gaussians 4 \\\n            2&gt;&gt; {log}\n          apply_indel=true\n        fi\n\n        if [ \"$apply_snp\" = true ]; then\n          {GATK4} ApplyVQSR \\\n            -R {REF} -V \"$tmp_vcf\" \\\n            --recal-file {VARCALLDIR}/{ID}.hc.snp.recal.vcf.gz \\\n            --tranches-file {VARCALLDIR}/{ID}.hc.snp.tranches.txt \\\n            --mode SNP --truth-sensitivity-filter-level 99.0 \\\n            -O {VARCALLDIR}/{ID}.hc.post_snp.vcf.gz \\\n            2&gt;&gt; {log}\n          tmp_vcf=\"{VARCALLDIR}/{ID}.hc.post_snp.vcf.gz\"\n        fi\n\n        if [ \"$apply_indel\" = true ]; then\n          {GATK4} ApplyVQSR \\\n            -R {REF} -V \"$tmp_vcf\" \\\n            --recal-file {VARCALLDIR}/{ID}.hc.indel.recal.vcf.gz \\\n            --tranches-file {VARCALLDIR}/{ID}.hc.indel.tranches.txt \\\n            --mode INDEL --truth-sensitivity-filter-level 95.0 \\\n            -O {VARCALLDIR}/{ID}.hc.vqsr.vcf.gz \\\n            2&gt;&gt; {log}\n          tmp_vcf=\"{VARCALLDIR}/{ID}.hc.vqsr.vcf.gz\"\n        fi\n\n        {GATK4} VariantFiltration \\\n          -R {REF} \\\n          -V \"$tmp_vcf\" \\\n          --filter-name \"LowQUAL\" --filter-expression \"QUAL &lt; 30.0\" \\\n          --filter-name \"QD2\"        --filter-expression \"QD &lt; 2.0\" \\\n          --filter-name \"FS60\"       --filter-expression \"FS &gt; 60.0\" \\\n          --filter-name \"MQ40\"       --filter-expression \"MQ &lt; 40.0\" \\\n          --filter-name \"MQRS-12.5\"  --filter-expression \"MQRankSum &lt; -12.5\" \\\n          --filter-name \"RPRS-8\"     --filter-expression \"ReadPosRankSum &lt; -8.0\" \\\n          --filter-name \"QD2_indel\"  --filter-expression \"QD &lt; 2.0\" \\\n          --filter-name \"FS200\"      --filter-expression \"FS &gt; 200.0\" \\\n          --filter-name \"RPRS-20\"    --filter-expression \"ReadPosRankSum &lt; -20.0\" \\\n          -O {output.qc} \\\n          2&gt;&gt; {log}\n        \"\"\"\n\n# -----------------------------\n# STEP 10: Coverage\n# -----------------------------\nrule coverage_stats:\n    input:\n        raw   = os.path.join(BAMDIR, f\"{ID}.rg.merged.dedup.bam\"),\n        recal = os.path.join(BAMDIR, f\"{ID}.rg.merged.dedup.recal.bam\")\n    output:\n        cov   = os.path.join(STATSDIR, f\"{ID}.coverage.txt\")\n    log:\n        os.path.join(LOGDIR, f\"{ID}.08_coverage_stats.log\")\n    shell:\n        r\"\"\"\n        set -eu\n\n        chrN=\"{CHR1}\"\n        bam_raw=\"{input.raw}\"\n        bam_recal=\"{input.recal}\"\n\n        out_raw=\"{STATSDIR}/{CHR1}.raw.bam\"\n        out_dedup=\"{STATSDIR}/{CHR1}.dedup.bam\"\n\n        {SAM} view -b \"$bam_raw\"   \"$chrN\" &gt; \"$out_raw\"   2&gt;&gt; {log}\n        {SAM} view -b \"$bam_recal\" \"$chrN\" &gt; \"$out_dedup\" 2&gt;&gt; {log}\n\n        {SAM} index \"$out_raw\"   2&gt;&gt; {log}\n        {SAM} index \"$out_dedup\" 2&gt;&gt; {log}\n\n        bash {COV} {ID} \"$out_raw\" \"$out_dedup\" {PIPELINE} \\\n          &gt; {output.cov} 2&gt;&gt; {log}\n\n        rm -f \"$out_raw\" \"$out_dedup\" \"$out_raw.bai\" \"$out_dedup.bai\"\n        \"\"\"\n\n# -----------------------------\n# STEP 10 cont: Sex\n# -----------------------------\nrule sex_determination:\n    input:\n        qc=os.path.join(VARCALLDIR, f\"{ID}.hc.QC.vcf.gz\")\n    output:\n        sex=os.path.join(STATSDIR, f\"{ID}.sex.txt\")\n    log:\n        os.path.join(LOGDIR, f\"{ID}.09_sex_determination.log\")\n    shell:\n        r\"\"\"\n        set -eu\n        bash {VCF2SEX} {input.qc} &gt; {output.sex} 2&gt;&gt; {log}\n        \"\"\"\n\n# -----------------------------\n# STEP 11: Optional cleanup\n# -----------------------------\nrule cleanup_bams:\n    input:\n        os.path.join(VARCALLDIR, f\"{ID}.hc.QC.vcf.gz\")\n    output:\n        done=os.path.join(LOGDIR, f\"{ID}.cleanup.done\")\n    log:\n        os.path.join(LOGDIR, f\"{ID}.10_cleanup_bams.log\")\n    run:\n        # write any python errors to the log file\n        try:\n            if CLEANUP_BAM:\n                for pat in (\"01_bam/*.bam\", \"01_bam/*.bai\"):\n                    for fp in glob.glob(pat):\n                        try:\n                            os.remove(fp)\n                        except FileNotFoundError:\n                            pass\n            Path(output.done).write_text(\"ok\\n\")\n        except Exception as e:\n            Path(log[0]).write_text(str(e) + \"\\n\")\n            raise\n</code></pre>"},{"location":"pipelines/wes_wgs_single/#diagram-single-sample-weswgs-workflow","title":"Diagram: Single-Sample WES/WGS Workflow","text":"flowchart TD     A[\"Input FASTQ reads\"]     B[\"Align with BWA MEM and add read groups\"]     C[\"Merge lane BAM files\"]     D[\"Mark duplicates\"]     E[\"Base recalibration BQSR\"]     F[\"Run HaplotypeCaller to produce gVCF\"]     G[\"Run GenotypeGVCFs to produce raw VCF\"]     H[\"Check if enough variants for VQSR\"]     I[\"Build SNP VQSR model\"]     J[\"Build INDEL VQSR model\"]     K[\"Apply VQSR\"]     L[\"Apply hard filters\"]     M[\"Write final QC VCF\"]     N[\"Compute coverage and infer sex\"]      A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H     H --&gt;|Yes| I --&gt; J --&gt; K --&gt; M     H --&gt;|No| L --&gt; M     M --&gt; N"},{"location":"pipelines/wes_wgs_single/#purpose","title":"Purpose","text":"<p>This pipeline processes one sample at a time and produces a high-quality, filtered VCF suitable for downstream analysis. It automatically adapts to:</p> <ul> <li>WES: restricted to an exome interval list  </li> <li>WGS: whole genome (no interval restriction)</li> </ul>"},{"location":"pipelines/wes_wgs_single/#what-the-pipeline-does","title":"What the Pipeline Does","text":""},{"location":"pipelines/wes_wgs_single/#1-alignment-read-groups","title":"1. Alignment &amp; Read Groups","text":"<ul> <li>Align paired-end FASTQ files using BWA-MEM.</li> <li>Add read groups (sample, library, lane, platform) required by GATK.</li> <li>Output: lane-level BAMs with correct RG tags.</li> </ul>"},{"location":"pipelines/wes_wgs_single/#2-lane-merging","title":"2. Lane Merging","text":"<ul> <li>Merge all lane BAMs for the same sample into a single BAM.</li> <li>Ensures duplicate marking and BQSR operate on the full dataset.</li> </ul>"},{"location":"pipelines/wes_wgs_single/#3-duplicate-marking","title":"3. Duplicate Marking","text":"<ul> <li>Use GATK <code>MarkDuplicates</code> on the merged BAM.</li> <li>Flags PCR/optical duplicates to prevent them from inflating support for artefactual variants.</li> </ul>"},{"location":"pipelines/wes_wgs_single/#4-base-quality-score-recalibration-bqsr","title":"4. Base Quality Score Recalibration (BQSR)","text":"<ul> <li>Two-step process: <code>BaseRecalibrator</code> then <code>ApplyBQSR</code>.</li> <li>Uses known variant databases (dbSNP, Mills, 1000G indels) to model and correct systematic base-quality errors.</li> <li>Output: recalibrated BAM used for variant calling.</li> </ul>"},{"location":"pipelines/wes_wgs_single/#5-variant-calling-haplotypecaller-gvcf","title":"5. Variant Calling (HaplotypeCaller, gVCF)","text":"<ul> <li>Run GATK <code>HaplotypeCaller</code> in GVCF mode (<code>-ERC GVCF</code>).</li> <li>WES: uses exome intervals; WGS: full genome.</li> <li>Output: <code>&lt;id&gt;.hc.g.vcf.gz</code> (per-sample gVCF).</li> </ul>"},{"location":"pipelines/wes_wgs_single/#6-genotypegvcfs-raw-vcf","title":"6. GenotypeGVCFs (Raw VCF)","text":"<ul> <li>Run GATK <code>GenotypeGVCFs</code> on the sample gVCF.</li> <li>Output: <code>&lt;id&gt;.hc.raw.vcf.gz</code> (raw VCF with SNPs and indels).</li> </ul>"},{"location":"pipelines/wes_wgs_single/#7-variant-quality-score-recalibration-vqsr","title":"7. Variant Quality Score Recalibration (VQSR)","text":"<ul> <li>If there are enough variants (SNPs and indels), build VQSR models:</li> <li><code>VariantRecalibrator</code> for SNPs and indels separately.</li> <li>Uses multiple annotations (QD, MQ, FS, MQRankSum, ReadPosRankSum).</li> <li>Output: recalibration VCFs and tranche files.</li> </ul>"},{"location":"pipelines/wes_wgs_single/#8-apply-vqsr-or-hard-filters","title":"8. Apply VQSR or Hard Filters","text":"<ul> <li>If models exist:</li> <li>Apply SNP VQSR.</li> <li>Then apply INDEL VQSR.</li> <li>Output: <code>&lt;id&gt;.hc.vqsr.vcf.gz</code>.</li> <li>If not:</li> <li>Skip directly to hard filters on the raw VCF or post-SNP VQSR VCF.</li> </ul>"},{"location":"pipelines/wes_wgs_single/#9-generate-final-qc-vcf","title":"9. Generate Final QC VCF","text":"<ul> <li>Run <code>VariantFiltration</code> with recommended hard filters on annotations.</li> <li>Output: <code>&lt;id&gt;.hc.QC.vcf.gz</code> (final QC VCF).</li> </ul>"},{"location":"pipelines/wes_wgs_single/#10-coverage-sex-determination","title":"10. Coverage &amp; Sex Determination","text":"<ul> <li>Extract chromosome 1 reads from raw and recalibrated BAMs.</li> <li>Compute coverage statistics.</li> <li>Infer sample sex from final VCF using a dedicated script.</li> <li>Outputs:</li> <li><code>03_stats/&lt;id&gt;.coverage.txt</code></li> <li><code>03_stats/&lt;id&gt;.sex.txt</code></li> </ul>"},{"location":"pipelines/wes_wgs_single/#output-files","title":"Output Files","text":"File Meaning <code>02_varcall/&lt;id&gt;.hc.g.vcf.gz</code> Per-sample gVCF (HaplotypeCaller) <code>02_varcall/&lt;id&gt;.hc.raw.vcf.gz</code> Raw VCF after GenotypeGVCFs <code>02_varcall/&lt;id&gt;.hc.vqsr.vcf.gz</code> VCF after VQSR (if VQSR was applied) <code>02_varcall/&lt;id&gt;.hc.QC.vcf.gz</code> Final QC-filtered VCF (recommended) <code>03_stats/&lt;id&gt;.coverage.txt</code> Coverage metrics <code>03_stats/&lt;id&gt;.sex.txt</code> Sex determination result <code>logs/&lt;id&gt;.log</code> Main pipeline log"},{"location":"pipelines/wes_wgs_single/#when-to-use-this-pipeline","title":"When to Use This Pipeline","text":"<ul> <li>Standard clinical or research WES or WGS processing.</li> <li>Generating gVCFs for cohort joint genotyping.</li> <li>Producing high-quality single-sample VCFs for interpretation.</li> </ul>"},{"location":"technical-details/adding-a-pipeline/","title":"Adding a new pipeline","text":"<p>CBIcall is designed to be extensible: you can add new analysis pipelines without changing the core framework as long as the workflow wiring is declared in the workflow registry.</p> <p>In CBIcall, the source of truth for available workflows is:</p> <ul> <li><code>workflows/config/cbicall.workflows.yaml</code> (registry; what exists)</li> <li><code>workflows/schema/workflows.schema.json</code> (schema; what a valid registry looks like)</li> </ul> <p>The Python code validates user parameters (enums, defaults, compatibility rules) and then resolves the correct workflow script from the YAML registry.</p>"},{"location":"technical-details/adding-a-pipeline/#1-how-cbicall-discovers-workflows","title":"1. How CBIcall discovers workflows","text":"<p>At runtime, CBIcall:</p> <ol> <li>Reads the user parameter YAML (e.g. <code>params.yaml</code>)</li> <li>Applies defaults and validates semantic rules (pipeline/mode/genome/engine compatibility)</li> <li>Loads and schema-validates the workflow registry:</li> <li><code>workflows/config/cbicall.workflows.yaml</code></li> <li><code>workflows/schema/workflows.schema.json</code></li> <li>Resolves workflow scripts from the registry and checks:</li> <li>referenced files exist</li> <li>Bash scripts are executable (<code>+x</code>)</li> <li>Dispatches the workflow through the selected engine (Bash or Snakemake)</li> </ol> <p>Key point: adding a pipeline is mostly YAML + scripts, not Python.</p>"},{"location":"technical-details/adding-a-pipeline/#2-create-workflow-scripts","title":"2. Create workflow scripts","text":"<p>Create one or more workflow entrypoints under the engine directory and GATK version:</p>"},{"location":"technical-details/adding-a-pipeline/#bash","title":"Bash","text":"<pre><code>workflows/bash/gatk-4.6/\n  mypipe_single.sh\n  mypipe_cohort.sh\n  parameters.sh\n</code></pre>"},{"location":"technical-details/adding-a-pipeline/#snakemake","title":"Snakemake","text":"<pre><code>workflows/snakemake/gatk-4.6/\n  mypipe_single.smk\n  mypipe_cohort.smk\n  config.yaml\n</code></pre>"},{"location":"technical-details/adding-a-pipeline/#naming-conventions","title":"Naming conventions","text":"<p>CBIcall follows the pattern:</p> <ul> <li><code>{pipeline}_{mode}.{sh|smk}</code></li> <li><code>pipeline</code>: e.g. <code>wes</code>, <code>wgs</code>, <code>mit</code>, <code>mypipe</code></li> <li><code>mode</code>: <code>single</code> or <code>cohort</code></li> </ul>"},{"location":"technical-details/adding-a-pipeline/#3-register-the-pipeline-in-the-workflow-registry","title":"3. Register the pipeline in the workflow registry","text":"<p>Edit <code>workflows/config/cbicall.workflows.yaml</code> and add your pipeline under the appropriate engine + version.</p>"},{"location":"technical-details/adding-a-pipeline/#minimal-example-bash-single-mode","title":"Minimal example (Bash, single mode)","text":"<pre><code>workflows:\n  bash:\n    base_dir: \"workflows/bash\"\n    versions:\n      gatk-4.6:\n        common:\n          parameters: \"parameters.sh\"\n          coverage: \"coverage.sh\"\n          jaccard: \"jaccard.sh\"\n          vcf2sex: \"vcf2sex.sh\"\n        pipelines:\n          mypipe:\n            single: \"mypipe_single.sh\"\n</code></pre>"},{"location":"technical-details/adding-a-pipeline/#adding-cohort-mode","title":"Adding cohort mode","text":"<pre><code>        pipelines:\n          mypipe:\n            single: \"mypipe_single.sh\"\n            cohort: \"mypipe_cohort.sh\"\n</code></pre>"},{"location":"technical-details/adding-a-pipeline/#snakemake-example","title":"Snakemake example","text":"<pre><code>workflows:\n  bash:\n    base_dir: \"workflows/bash\"\n    versions:\n      gatk-4.6:\n        common:\n          parameters: \"parameters.sh\"\n          coverage: \"coverage.sh\"\n          jaccard: \"jaccard.sh\"\n          vcf2sex: \"vcf2sex.sh\"\n        pipelines:\n          wes:\n            single: \"wes_single.sh\"\n\n  snakemake:\n    base_dir: \"workflows/snakemake\"\n    versions:\n      gatk-4.6:\n        common:\n          config: \"config.yaml\"\n        pipelines:\n          mypipe:\n            single: \"mypipe_single.smk\"\n</code></pre> <p>The schema currently requires <code>workflows.bash</code> to exist, even if you primarily use Snakemake. Keep a minimal bash block if needed.</p>"},{"location":"technical-details/adding-a-pipeline/#4-expose-user-parameters-via-the-parameter-yaml","title":"4. Expose user parameters via the parameter YAML","text":"<p>Users select the workflow by providing:</p> <pre><code>pipeline: mypipe\nmode: single\nworkflow_engine: bash\ngatk_version: gatk-4.6\nprojectdir: cbicall\n</code></pre> <p>Then add pipeline-specific fields as needed (reference, targets, etc.). The recommended approach is:</p> <ul> <li>keep \u201cpolicy\u201d options in YAML (things the user should control)</li> <li>keep script-specific internal wiring inside the workflow scripts</li> </ul>"},{"location":"technical-details/adding-a-pipeline/#5-single-vs-cohort-mode","title":"5. Single vs cohort mode","text":"<p>If you support both modes, register both in the YAML and provide both entrypoints.</p> <p>If cohort mode does not make sense, omit it from the registry. CBIcall will raise a clear error if a user requests an unavailable mode.</p>"},{"location":"technical-details/adding-a-pipeline/#6-validation-and-guardrails","title":"6. Validation and guardrails","text":"<p>CBIcall will fail fast when:</p> <ul> <li><code>pipeline</code>, <code>mode</code>, <code>workflow_engine</code>, <code>gatk_version</code> are invalid</li> <li>the pipeline/mode combination is not allowed for the selected GATK version</li> <li>incompatible combinations are selected (example: <code>snakemake</code> with <code>gatk-3.5</code> if restricted)</li> <li>a registry entry points to a missing file</li> <li>a Bash workflow script is not executable (<code>+x</code>)</li> </ul> <p>This keeps \u201cwhat is allowed\u201d stable and \u201cwhat is wired\u201d configurable.</p>"},{"location":"technical-details/adding-a-pipeline/#7-recommended-add-a-minimal-example-dataset","title":"7. Recommended: Add a minimal example dataset","text":"<p>Create a tiny, deterministic example to support CI and user onboarding:</p> <pre><code>examples/mypipe_test/\n  params.yaml\n  input/\n  expected/\n</code></pre> <p>Goals:</p> <ul> <li>fast execution (ideally &lt; 1 minute)</li> <li>deterministic outputs</li> <li>validates one or two key artifacts</li> </ul>"},{"location":"technical-details/adding-a-pipeline/#8-document-the-pipeline-page","title":"8. Document the pipeline page","text":"<p>Add a new page:</p> <pre><code>docs/pipelines/mypipe.md\n</code></pre> <p>Include:</p> <ul> <li>requirements (tools, references)</li> <li>supported modes</li> <li>required YAML parameters</li> <li>example command(s)</li> <li>output layout</li> </ul> <p>Then add it to <code>mkdocs.yml</code>:</p> <pre><code>nav:\n  - Pipelines:\n      - Adding a pipeline: pipelines/adding-a-pipeline.md\n      - MyPipe: pipelines/mypipe.md\n</code></pre>"},{"location":"technical-details/adding-a-pipeline/#9-optional-container-support","title":"9. Optional: Container support","text":"<p>If the pipeline requires extra software, pin versions for reproducibility:</p> <ul> <li>extend your Dockerfile, or</li> <li>publish a new container image</li> </ul> <p>Make sure the workflow scripts remain runnable in the container environment.</p>"},{"location":"technical-details/adding-a-pipeline/#quick-checklist","title":"Quick checklist","text":"<ul> <li> Add workflow script(s) under <code>workflows/{bash|snakemake}/{gatk-version}/</code></li> <li> Register pipeline in <code>workflows/config/cbicall.workflows.yaml</code></li> <li> Ensure registry passes <code>workflows/schema/workflows.schema.json</code></li> <li> Make Bash scripts executable (<code>chmod +x</code>)</li> <li> Add example <code>params.yaml</code></li> <li> Add docs page and link it in <code>mkdocs.yml</code></li> </ul>"},{"location":"technical-details/architecture/","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"technical-details/architecture/#overview","title":"Overview","text":"<p>CBIcall is a thin orchestration layer around one or more concrete pipelines. Its main responsibilities are:</p> <ul> <li>Reading a YAML configuration file</li> <li>Validating required parameters and paths</li> <li>Selecting the pipeline and workflow engine</li> <li>Preparing the project directory structure</li> <li>Calling the appropriate workflow scripts (Bash or Snakemake)</li> <li>Managing logs and collecting results in a standard layout</li> </ul> <p>The actual bioinformatics work (alignment, variant calling, QC) is implemented in modular pipelines that can be extended or replaced.</p>"},{"location":"technical-details/architecture/#main-components","title":"Main components","text":"<p>At a high level, CBIcall consists of:</p> <ul> <li> <p>Python wrapper   Parses the YAML configuration, validates parameters, resolves paths and dispatches to the selected pipeline and engine.</p> </li> <li> <p>Pipelines   Implement the domain logic for WES, WGS and mtDNA runs. Each pipeline lives in its own directory and can provide Bash and/or Snakemake workflows.   Common parameters are loaded via <code>parameters.sh</code>.</p> </li> <li> <p>Workflow engines   Control execution:</p> </li> <li>Bash scripts for simple, transparent runs</li> <li> <p>Snakemake workflows for dependency tracking and parallelization</p> </li> <li> <p>Project layout and logs   A standard output structure with separate <code>01_bam/</code>, <code>02_varcall/</code>, <code>03_stats/</code> and <code>logs/</code> directories, reused across all pipelines.</p> </li> <li> <p>External data   Executables and databases for third-party tools, reference genomes and accessory data.</p> </li> </ul>"},{"location":"technical-details/architecture/#architecture-diagram","title":"Architecture diagram","text":"flowchart TD    U[Command-line user] --&gt; B[cbicall entry script]   B --&gt; W[Python wrapper]    W --&gt; C[Parse &amp; validate YAML]   W --&gt; S[\"Set up project directory (projectdir/, 01_bam/, 02_varcall/, 03_stats/, logs/)\"]    W --&gt; E{Select workflow engine}   E --&gt; BE[Bash engine]   E --&gt; SE[Snakemake engine]    BE --&gt; PB{Select pipeline - bash}   PB --&gt; BE_WES[wes_bash]   PB --&gt; BE_WGS[wgs_bash]   PB --&gt; BE_MIT[mit_bash]    SE --&gt; PS{Select pipeline - snakemake}   PS --&gt; SE_WES[wes_smk]   PS --&gt; SE_WGS[wgs_smk]   PS --&gt; SE_MIT[mit_smk]    BE_WES --&gt; O[Standard output structure]   BE_WGS --&gt; O   BE_MIT --&gt; O    SE_WES --&gt; O   SE_WGS --&gt; O   SE_MIT --&gt; O"},{"location":"technical-details/architecture/#directory-structure","title":"Directory structure","text":"<pre><code>&lt;projectdir&gt;/\n  01_bam/\n  02_varcall/\n  03_stats/\n  logs/\n</code></pre> <p>Typical usage:</p> <ul> <li>Intermediate alignment and BAM files are stored under <code>01_bam/</code>.</li> <li>Variant-calling outputs (gVCFs, VCFs and related files) are stored under <code>02_varcall/</code>.</li> <li>Summary statistics and QC metrics are collected under <code>03_stats/</code>.</li> <li>Log files for all steps are stored under <code>logs/</code>.</li> </ul>"},{"location":"technical-details/architecture/#execution-model","title":"Execution model","text":"<p>CBIcall supports two main execution modes:</p> <ul> <li> <p>Single mode   Each sample is processed independently.</p> </li> <li> <p>Cohort mode   Joint analysis using per-sample gVCFs from previous single runs.</p> </li> </ul> <p>The workflow engine is selected in the YAML:</p> <ul> <li><code>workflow_engine: bash</code> </li> <li><code>workflow_engine: snakemake</code></li> </ul>"},{"location":"technical-details/architecture/#supported-pipelines","title":"Supported pipelines","text":"<p>The following table shows valid pipeline and mode combinations for each GATK version:</p> GATK Version wes_single wes_cohort wgs_single wgs_cohort mit_single mit_cohort gatk-3.5 + + - - + + gatk-4.6 + + + + - - <p>Date: Oct-2025</p>"},{"location":"technical-details/architecture/#extensibility","title":"Extensibility","text":"<p>New pipelines can be added without modifying the core system:</p> <ul> <li>Each pipeline lives under <code>workflows/&lt;name&gt;/</code></li> <li>The wrapper maps <code>pipeline: &lt;name&gt;</code> to the correct implementation</li> <li>Pipelines reuse the same directory layout and logging conventions</li> <li>Pipelines may support single and/or cohort mode</li> </ul> <p>See:</p> <p>\u27a1\ufe0f Extend Pipelines</p>"},{"location":"usage/end-to-end-example-mit/","title":"End-to-end examples (MToolBox)","text":"<p>Prerequisites</p> <p>Installation, reference bundles, and all dependencies must be completed beforehand.  </p> <p>\u27a1\ufe0f Installation</p> <p>Architecture</p> <p>MToolBox supports x86_64 only. ARM-based systems, including Apple Silicon (M1/M2/M3), are not supported.</p> MIT single-sample runMIT cohort run"},{"location":"usage/end-to-end-example-mit/#1-before-running-mtdna-calling-you-must-have-a-bam-file-coming-from-weswgs","title":"1. Before running mtDNA calling you must have a <code>bam</code> file coming from wes/wgs","text":"<p>Does it matter if I ran WES/WGS with GATK 3.5 or GATK 4.6?</p> <p>No. CBIcall will detect and use the <code>bam</code> files produced by either version. Just make sure that <code>bam</code> files are available \u2014 FASTQ input is not supported.</p> <p>CBIcall expects BAM file infrom a previous run:</p> <pre><code>CNAG999_exome\n\u2514\u2500\u2500 CNAG99901P_ex  &lt;--- ID taken from here\n    \u2514\u2500\u2500 cbicall_bash_w?s_single_gatk-* &lt;- The script expects that you have a BAM file inside this directory\n</code></pre> <p>Note on nomenclature</p> <p>Please see this page.</p>"},{"location":"usage/end-to-end-example-mit/#2-create-a-parameters-file","title":"2. Create a parameters file","text":"<p>Create a YAML file, e.g. <code>mit_single.yaml</code>:</p> <pre><code>mode:            single\npipeline:        mit\nworkflow_engine: bash\ngatk_version:    gatk-3.5\nsample:          CNAG999_exome/CNAG99901P_ex\n</code></pre>"},{"location":"usage/end-to-end-example-mit/#3-run-cbicall","title":"3. Run CBIcall","text":"<pre><code>bin/cbicall -p mit_single.yaml -t 4\n</code></pre> <ul> <li><code>-p</code> selects the YAML parameters file  </li> <li><code>-t</code> sets the number of threads</li> </ul>"},{"location":"usage/end-to-end-example-mit/#4-inspect-outputs","title":"4. Inspect outputs","text":"<p>After completion, you will find:</p> <pre><code>CNAG999_exome/CNAG99901P_ex/cbicall_bash_wes_mit_gatk-3.5_*/\n  01_mtoolbox/\n  02_browser/\n</code></pre>"},{"location":"usage/end-to-end-example-mit/#5-visualize-variants-in-the-browser","title":"5.  Visualize variants in the browser","text":"<p>Please see:</p> <pre><code>02_browser/README.txt\n</code></pre> <p>The results are reported both as a HTML table and as downloadable files.</p> <p>See snapshot</p> <p></p>"},{"location":"usage/end-to-end-example-mit/#downloadable-files","title":"Downloadable files:","text":"<ul> <li>mtDNA JSON A JSON file with the results from <code>mit_prioritized_variants.txt</code>.</li> <li>Report: A tsv file including all the annotations for each variant. Name of the file <code>mit_prioritized_variants.txt</code>.</li> <li>Haplog: A tsv file including the predicted haplogroup for each sample. Name of the file <code>mt_classification_best_results.csv</code>.</li> <li>VCF: A text file consisting of all the variants in the VCF format. Name of the file <code>VCF_file.vcf</code>.</li> </ul>"},{"location":"usage/end-to-end-example-mit/#html-table","title":"HTML table:","text":"<p>In this tab SG-ADVISER mtDNA displays a browsable table consisting of the most relavant fields relative to the variant annotation:</p> <ul> <li>Sample: The full name of each sample.</li> <li>Locus: The location on the mitochondrial chromosome.</li> <li>Variant_Allele: The position in the mitochondrial chromosome + the alternative allele format.</li> <li>Ref: The reference allele (mitochondrial reference genome: RSRS).</li> <li>Alt: The alternative allele(s).</li> <li>Aa_change: The amino acid change if the variant falls in a coding region.</li> <li>GT: Genotype. 0:Ref, \u22651:Alt(s).</li> <li>Depth: The number of times this position is covered by reads.</li> <li>Heterop_Frac: The heteroplasmic fraction. Note that the confidence interval can be retrieved from the downloadable VCF file.</li> <li>Other: For other fields please consult MToolBox's manual.</li> </ul> <p>For advanced parameters, multi-sample analyses, mtDNA workflows and troubleshooting, see the Usage and FAQ sections.</p>"},{"location":"usage/end-to-end-example-mit/#1-before-running-mtdna-calling-you-must-have-bam-files-coming-from-weswgs","title":"1. Before running mtDNA calling you must have <code>bam</code> files coming from wes/wgs","text":"<p>Does it matter if I ran WES/WGS with GATK 3.5 or GATK 4.6?</p> <p>No. CBIcall will detect and use the <code>bam</code> files produced by either version. Just make sure that <code>bam</code> files are available \u2014 FASTQ input is not supported.</p> <p>CBIcall expects BAM file from previous runs:</p> <pre><code>CNAG999_exome\n\u2514\u2500\u2500 CNAG99901P_ex  &lt;--- ID taken from here\n    \u2514\u2500\u2500 cbicall_bash_w?s_single_gatk-* &lt;- The script expects that you have a BAM file inside this directory\n</code></pre> <p>Note on nomenclature</p> <p>Please see this page.</p>"},{"location":"usage/end-to-end-example-mit/#2-create-a-parameters-file_1","title":"2. Create a parameters file","text":"<p>Create a YAML file, e.g. <code>mit_cohort.yaml</code>:</p> <pre><code>mode:            cohort\npipeline:        mit\nworkflow_engine: bash\ngatk_version:    gatk-3.5\nsample:          CNAG999_exome\n</code></pre>"},{"location":"usage/end-to-end-example-mit/#3-run-cbicall_1","title":"3. Run CBIcall","text":"<pre><code>bin/cbicall -p mit_cohort.yaml -t 4\n</code></pre> <ul> <li><code>-p</code> selects the YAML parameters file  </li> <li><code>-t</code> sets the number of threads</li> </ul>"},{"location":"usage/end-to-end-example-mit/#4-inspect-outputs_1","title":"4. Inspect outputs","text":"<p>After completion, you will find:</p> <pre><code>CNAG999_exome/cbicall_bash_mit_cohort_rsrs_gatk-3.5*\n  01_mtoolbox/\n  02_browser/\n</code></pre>"},{"location":"usage/end-to-end-example-mit/#5-visualize-variants-in-the-browser_1","title":"5.  Visualize variants in the browser","text":"<p>Please see:</p> <pre><code>02_browser/README.txt\n</code></pre> <p>The results are reported both as a HTML table and as downloadable files.</p> <p>See snapshot</p> <p></p>"},{"location":"usage/end-to-end-example-mit/#downloadable-files_1","title":"Downloadable files:","text":"<ul> <li>mtDNA JSON A JSON file with the results from <code>mit_prioritized_variants.txt</code>.</li> <li>Report: A tsv file including all the annotations for each variant. Name of the file <code>mit_prioritized_variants.txt</code>.</li> <li>Haplog: A tsv file including the predicted haplogroup for each sample. Name of the file <code>mt_classification_best_results.csv</code>.</li> <li>VCF: A text file consisting of all the variants in the VCF format. Name of the file <code>VCF_file.vcf</code>.</li> </ul>"},{"location":"usage/end-to-end-example-mit/#html-table_1","title":"HTML table:","text":"<p>In this tab SG-ADVISER mtDNA displays a browsable table consisting of the most relavant fields relative to the variant annotation:</p> <ul> <li>Sample: The full name of each sample.</li> <li>Locus: The location on the mitochondrial chromosome.</li> <li>Variant_Allele: The position in the mitochondrial chromosome + the alternative allele format.</li> <li>Ref: The reference allele (mitochondrial reference genome: RSRS).</li> <li>Alt: The alternative allele(s).</li> <li>Aa_change: The amino acid change if the variant falls in a coding region.</li> <li>GT: Genotype. 0:Ref, \u22651:Alt(s).</li> <li>Depth: The number of times this position is covered by reads.</li> <li>Heterop_Frac: The heteroplasmic fraction. Note that the confidence interval can be retrieved from the downloadable VCF file.</li> <li>Other: For other fields please consult MToolBox's manual.</li> </ul>"},{"location":"usage/end-to-end-example-wes/","title":"End-to-end examples (GATK 4.6)","text":"<p>Prerequisites</p> <p>Installation, reference bundles, and all dependencies must be completed beforehand.  </p> <p>\u27a1\ufe0f Installation</p> WES single-sample runWES cohort run <p>This example demonstrates how to run CBIcall on a real WES sample from FASTQ files through final VCF and QC outputs.</p> <p>Important</p> <p>In order to run a <code>cohort</code> based calculation you first have to create <code>GVCF</code> for each sample. This is being done by running <code>wes</code> mode <code>single</code>.</p>"},{"location":"usage/end-to-end-example-wes/#1-prepare-your-fastq-files","title":"1. Prepare your FASTQ files","text":"<p>CBIcall expects paired-end FASTQ files with a shared prefix, for example:</p> <pre><code># Project    / Sample (Proband WES)\nCNAG999_exome/CNAG99901P_ex/\n  CNAG99901P_ex_S1_L001_R1_001.fastq.gz\n  CNAG99901P_ex_S1_L001_R2_001.fastq.gz\n</code></pre> <p>Note on nomenclature</p> <p>Please see this page.</p>"},{"location":"usage/end-to-end-example-wes/#2-create-a-parameters-file","title":"2. Create a parameters file","text":"<p>Create a YAML file, e.g. <code>wes_single.yaml</code>:</p> <pre><code>mode:            single\npipeline:        wes\nworkflow_engine: bash\ngatk_version:    gatk-4.6\nsample:          CNAG999_exome/CNAG99901P_ex\ngenome:          b37\ncleanup_bam:     false\n</code></pre> <p>Notes:</p> <ul> <li><code>mode</code> selects single-sample or cohort (joint genotyping).  </li> <li><code>pipeline</code> switches between WES, WGS or mtDNA.  </li> <li><code>workflow_engine</code> chooses the backend (bash or snakemake).  </li> </ul>"},{"location":"usage/end-to-end-example-wes/#3-run-cbicall","title":"3. Run CBIcall","text":"<pre><code>bin/cbicall -p wes_single.yaml -t 4\n</code></pre> <ul> <li><code>-p</code> selects the YAML parameters file  </li> <li><code>-t</code> sets the number of threads</li> </ul>"},{"location":"usage/end-to-end-example-wes/#4-inspect-outputs","title":"4. Inspect outputs","text":"<p>After completion, you will find:</p> <pre><code>CNAG999_exome/CNAG99901P_ex/cbicall_bash_wes_single_gatk-4.6_*/\n  01_bam/\n  02_varcall/\n  03_stats/\n  logs/\n</code></pre> <p>Where:</p> <ul> <li>VCF files are stored in <code>02_varcall/</code> </li> <li>QC metrics (coverage, sample stats, sex prediction) are in <code>03_stats</code> </li> <li>Logs for all pipeline steps are under <code>logs/</code> </li> </ul> <p>These files are ready for downstream analysis, annotation or integration with cohort-level studies.</p> <p>For advanced parameters, multi-sample analyses, mtDNA workflows and troubleshooting, see the Usage and FAQ sections.</p>"},{"location":"usage/end-to-end-example-wes/#1-create-a-sample-map-file-like-the-one-we-display-below","title":"1. Create a sample map file like the one we display below:","text":"<pre><code>CNAG99901P_ex   /media/mrueda/2TBS/CNAG/Project_CBI_Call/cbicall/examples/input/CNAG999_exome/CNAG99901P_ex/ref_cbicall_bash_wes_single_b37_gatk-4.6_765963065360466/02_varcall/CNAG99901P.hc.QC.vcf.gz\nCNAG99902P_ex   /media/mrueda/2TBS/CNAG/Project_CBI_Call/cbicall/examples/input/CNAG999_exome/CNAG99901P_ex/ref_cbicall_bash_wes_single_b37_gatk-4.6_765963065360466/02_varcall/CNAG99901P.hc.QC.vcf.gz\n</code></pre> <p>GATK needs absolute paths for the files.</p>"},{"location":"usage/end-to-end-example-wes/#2-create-a-parameters-file_1","title":"2. Create a parameters file","text":"<p>Create a YAML file, e.g. <code>wes_cohort.yaml</code>:</p> <pre><code>mode:            cohort\npipeline:        wes\nworkflow_engine: bash\ngatk_version:    gatk-4.6\ngenome:          b37\nsample_map:      ./sample_map.tsv\n</code></pre>"},{"location":"usage/end-to-end-example-wes/#3-run-cbicall_1","title":"3. Run CBIcall","text":"<pre><code>bin/cbicall -p wes_cohort.yaml -t 4\n</code></pre> <ul> <li><code>-p</code> selects the YAML parameters file  </li> <li><code>-t</code> sets the number of threads</li> </ul>"},{"location":"usage/end-to-end-example-wes/#4-inspect-outputs_1","title":"4. Inspect outputs","text":"<p>After completion, you will find:</p> <pre><code>cbicall_bash_wes_cohort_gatk-4.6_*/\n  02_varcall/\n  logs/\n</code></pre> <p>Where:</p> <ul> <li>Final VCF files are stored in <code>02_varcall/</code> </li> <li>Logs for all pipeline steps are under <code>logs/</code> </li> </ul>"},{"location":"usage/quickstart/","title":"Quickstart","text":"<p>This page provides a minimal introduction to using CBIcall. It focuses on basic commands, help options, and a simple built-in test run.</p>"},{"location":"usage/quickstart/#1-basic-usage","title":"1. Basic usage","text":"<p>You can check the command-line interface using:</p> <pre><code>bin/cbicall --help\n</code></pre> <p>To display the version:</p> <pre><code>bin/cbicall --version\n</code></pre>"},{"location":"usage/quickstart/#2-minimal-command-example","title":"2. Minimal command example","text":"<p>Once you have a parameters file (YAML), you can run:</p> <pre><code>bin/cbicall -p param.yaml -t 8\n</code></pre> <ul> <li><code>-p</code> selects the param file  </li> <li><code>-t</code> sets the number of threads</li> </ul> <p>This is the minimal invocation required for any workflow.</p>"},{"location":"usage/quickstart/#3-run-a-small-wes-job-workstation","title":"3. Run a small WES job (workstation)","text":"<p>CBIcall includes small test configurations to validate that your installation works.</p> <p>Example:</p> <pre><code>cd examples/input\n../../bin/cbicall -p wes_single.yaml -t 4\n</code></pre> <p>When the job completes successfully (&lt;2 min), you should see:</p> <p><pre><code>CNAG999_exome/CNAG99901P_ex/cbicall_bash_wes_single_gatk-4.6_*/\n  01_bam/\n  02_varcall/\n  03_stats/\n  logs/\n</code></pre> The final <code>VCF</code> will be located at:</p> <pre><code>02_varcall/CNAG99901P.hc.QC.vcf.gz\n</code></pre>"},{"location":"usage/quickstart/#4-run-a-small-mtdna-job-workstation","title":"4. Run a small mtDNA job (workstation)","text":"<p>Example:</p> <pre><code>cd examples/input # ommit if you are already there\n../../bin/cbicall -p mit_single.yaml -t 4\n</code></pre> <p>When the job completes successfully (~ 5 min), you should see:</p> <pre><code>CNAG999_exome/CNAG99901P_ex/cbicall_bash_mit_single_gatk-3.5_*/\n  01_mtoolbox/\n  02_browser/\n</code></pre> <p>We recommend you to check:</p> <pre><code>02_browser/README.txt\n</code></pre>"},{"location":"usage/quickstart/#5-next-steps","title":"5. Next steps","text":"<p>For a complete, real-case workflow demonstration, see:</p> <p>\u27a1\ufe0f End-to-end example WES</p>"},{"location":"usage/usage/","title":"\ud83d\udcd8 General Usage","text":"<p>CNAG Biomedical Informatics framework for variant calling</p> <p> </p> <p>\ud83d\udcd8 Documentation: cnag-biomedical-informatics.github.io/cbicall</p> <p>\ud83d\udc33 Docker Hub Image: hub.docker.com/r/manuelrueda/cbicall/tags</p>"},{"location":"usage/usage/#table-of-contents","title":"Table of contents","text":"<ul> <li>Description</li> <li>Name</li> <li>Synopsis</li> <li>Summary</li> <li>Installation</li> <li>Non-Containerized</li> <li>Containerized</li> <li>How to run cbicall</li> <li>Citation</li> <li>Author</li> <li>License</li> </ul>"},{"location":"usage/usage/#name","title":"Name","text":"<p>CBIcall: CNAG Biomedical Informatics Framework for Variant Calling on Illumina DNA-seq (germline) NGS Data.</p>"},{"location":"usage/usage/#synopsis","title":"Synopsis","text":"<pre><code>cbicall -p &lt;parameters_file.yaml&gt; -t &lt;n_threads&gt; [options]\n\nArguments:\n  -p|param          Parameters input file (YAML)\n  -t|threads        Number of CPUs/Cores/Threads\n\nOptions:\n  -debug            Debugging level (from 1 to 5; 5 is maximum verbosity)\n  -h|help           Brief help message\n  -man              Full documentation\n  -v                Show version information\n  -verbose          Enable verbose output\n  -nc|no-color      Do not print colors to STDOUT\n  -ne|no-emoji      Do not print emojis to STDOUT\n</code></pre>"},{"location":"usage/usage/#summary","title":"Summary","text":"<p>CBIcall (CNAG Biomedical Informatics framework for variant calling) is a computational framework designed for variant calling analysis using Illumina Next-Generation Sequencing (NGS) data.</p>"},{"location":"usage/usage/#how-to-run-cbicall","title":"How to run CBIcall","text":"<p>CBIcall execution requires:</p> <ul> <li> <p>Input Files</p> <p>A folder containing Paired-End FASTQ files (e.g., <code>MA00001_exome/MA0000101P_ex/*{R1,R2}*fastq.gz</code>).</p> <p>You have a <code>examples/input/</code> directory with input data that you can use for testing.</p> </li> <li> <p>Parameters File</p> <p>A YAML-formatted parameters file controlling pipeline execution.</p> </li> </ul> <p>Below are the parameters that can be customized, along with their default values. Parameters must be separated from their values by whitespace or tabs.</p>"},{"location":"usage/usage/#essential-parameters","title":"Essential Parameters","text":"<pre><code>mode:            single  \npipeline:        wes          \nsample:          undef        \nsample_map:      undef\nworkflow_engine:   bash\ngatk_version:      gatk-4.6\ngenome:            b37\ncleanup_bam:       false\n</code></pre>"},{"location":"usage/usage/#optional-parameters-currently-unused","title":"Optional Parameters (Currently Unused)","text":"<pre><code>organism:        Homo Sapiens        \ntechnology:      Illumina HiSeq\n</code></pre> <p>CBIcall will create a dedicated project directory (<code>cbicall_*</code>) to store analysis outputs. This design allows multiple independent runs concurrently without modifying original input files.</p> <p>Below is a detailed description of key parameters:</p> <ul> <li> <p>cleanup_bam</p> <p>Set it to <code>true</code> to delete <code>01_bam/*.{bam,bai}</code>.</p> </li> <li> <p>gatk_version</p> <p>Supported values: <code>gatk-3.5</code> or <code>gatk-4.6</code>.</p> </li> <li> <p>genome</p> <p>Supported values: <code>b37</code> (default), <code>hg38</code> or <code>rsrs</code> (mtDNA).</p> </li> <li> <p>mode</p> <p>Two modes are supported: <code>single</code> (default, for individual samples) and <code>cohort</code> (for family/cohort-based analyses).</p> </li> <li> <p>pipeline</p> <p>Specifies the analysis pipeline. Currently available options: <code>wes</code> (whole-exome sequencing), <code>wgs</code> (whole-genome sequencing) and <code>mit</code> (mitochondrial DNA analysis). Note: to run <code>cohort</code> analysis, first complete a <code>single</code> analysis for each sample.</p> </li> <li> <p>projectdir</p> <p>The prefix for dir name (e.g., 'cancer_sample_001'). Note that it can also contain a path (e.g., foo/cancer_sample_001).</p> <p>Note: Such directory will be always created below the sample directory. The script will automatically add an unique identifier to each job.</p> </li> <li> <p>sample</p> <p>Path (relative or absolute) to the directory containing FASTQ files for analysis. See the <code>examples</code> directory for detailed guidance.</p> <p>Example:</p> <p>examples/input/CNAG999_exome/CNAG99901P_ex</p> </li> <li> <p>sample_map (cohort-mode only)</p> <p>Path (relative or absolute) to the file containing the sample ids and the paths for the GVCF files</p> <p>See example here</p> </li> <li> <p>workflow_engine</p> <p>Supported workflow engines: <code>bash</code> or <code>snakemake</code>.</p> </li> </ul>"},{"location":"usage/usage/#example-commands","title":"Example Commands","text":"<pre><code>$ bin/cbicall -p param_file.yaml -t 8\n$ bin/cbicall -p param_file.yaml -t 4 -verbose\n$ bin/cbicall -p param_file.yaml -t 16 &gt; log 2&gt;&amp;1\n$ $path_to_cbicall/bin/cbicall -p param_file.yaml -t 8 -debug 5\n$ nohup bin/cbicall -p param_file.yaml -t 4 &amp;\n</code></pre>"},{"location":"usage/usage/#recommended-specifications","title":"Recommended specifications:","text":"<p>CBIcall is optimized for multi-core Linux desktop, workstation, or server environments.</p> <pre><code>* Works in amd64 and arm64 archs (M-based Macs).\n* Ideally a Debian-based distribution (Ubuntu or Mint), but any other (e.g., CentOS, OpenSUSE) should do as well (untested).\n* &gt;= 8 GB RAM.\n* &gt;= 4 CPU cores (Intel i7 or Xeon preferred).\n* &gt;= 250 GB HDD space.\n* Python &gt;= 3.8\n* Java 8 (install via C&lt;sudo apt install openjdk-8-jdk&gt;).\n* Snakemake (install via C&lt;pip3 install -r requirements.txt&gt;).\n</code></pre>"},{"location":"usage/usage/#citation","title":"Citation","text":"<p>CBIcall: a configuration-driven framework for variant calling in large DNA-seq cohorts. Manuscript In preparation.</p>"},{"location":"usage/usage/#author","title":"Author","text":"<p>Written by Manuel Rueda (mrueda). GitHub repository: https://github.com/CNAG-Biomedical-Informatics/cbicall.</p>"},{"location":"usage/usage/#copyright-and-license","title":"Copyright and license","text":"<p>Please see the included LICENSE file for distribution and usage terms.</p>"}]}